{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified .\\notMNIST_large.tar.gz\n",
      "Found and verified .\\notMNIST_small.tar.gz\n",
      ".\\notMNIST_large already present - Skipping extraction of .\\notMNIST_large.tar.gz.\n",
      "['.\\\\notMNIST_large\\\\A', '.\\\\notMNIST_large\\\\B', '.\\\\notMNIST_large\\\\C', '.\\\\notMNIST_large\\\\D', '.\\\\notMNIST_large\\\\E', '.\\\\notMNIST_large\\\\F', '.\\\\notMNIST_large\\\\G', '.\\\\notMNIST_large\\\\H', '.\\\\notMNIST_large\\\\I', '.\\\\notMNIST_large\\\\J']\n",
      ".\\notMNIST_small already present - Skipping extraction of .\\notMNIST_small.tar.gz.\n",
      "['.\\\\notMNIST_small\\\\A', '.\\\\notMNIST_small\\\\B', '.\\\\notMNIST_small\\\\C', '.\\\\notMNIST_small\\\\D', '.\\\\notMNIST_small\\\\E', '.\\\\notMNIST_small\\\\F', '.\\\\notMNIST_small\\\\G', '.\\\\notMNIST_small\\\\H', '.\\\\notMNIST_small\\\\I', '.\\\\notMNIST_small\\\\J']\n",
      ".\\notMNIST_large\\A.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\B.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\C.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\D.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\E.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\F.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\G.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\H.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\I.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\J.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\A.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\B.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\C.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\D.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\E.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\F.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\G.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\H.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\I.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\J.pickle already present - Skipping pickling.\n",
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n",
      "Compressed pickle size: 690800503\n",
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n",
      "Training: (200000, 784) (200000, 10)\n",
      "Validation: (10000, 784) (10000, 10)\n",
      "Testing: (10000, 784) (10000, 10)\n",
      "Accuracy at step 0: 0.11\n",
      "Adding run metadata for 10\n",
      "Accuracy at step 20: 0.33\n",
      "Adding run metadata for 30\n",
      "Accuracy at step 40: 0.68\n",
      "Adding run metadata for 50\n",
      "Accuracy at step 60: 0.77\n",
      "Adding run metadata for 70\n",
      "Accuracy at step 80: 0.83\n",
      "Adding run metadata for 90\n",
      "Accuracy at step 100: 0.83\n",
      "Adding run metadata for 110\n",
      "Accuracy at step 120: 0.86\n",
      "Adding run metadata for 130\n",
      "Accuracy at step 140: 0.86\n",
      "Adding run metadata for 150\n",
      "Accuracy at step 160: 0.83\n",
      "Adding run metadata for 170\n",
      "Accuracy at step 180: 0.88\n",
      "Adding run metadata for 190\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from six.moves import xrange\n",
    "from noMnist_dataset import *\n",
    "    \n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_boolean('fake_data', False, 'the fake data used for unit testing')\n",
    "flags.DEFINE_integer('max__steps', 200, 'the number of epoch')\n",
    "flags.DEFINE_integer('learning__rate', 0.0001, 'the learning rate of the model')\n",
    "flags.DEFINE_integer('dropout', 0.9, 'the value of the droup out')\n",
    "flags.DEFINE_string('data__dir', '/input_data', 'the direction of the data')\n",
    "flags.DEFINE_string('log__dir', 'D:/Data Minning/train_code/train/noMnist/model/', 'the direction the log file')\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "def train():\n",
    "    \n",
    "    image_size = 28  # Pixel width and height.\n",
    "    pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "    def load_letter(folder, min_num_images):\n",
    "        \"\"\"Load the data for a single letter label.\"\"\"\n",
    "        image_files = os.listdir(folder)\n",
    "        dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "        print(folder)\n",
    "        num_images = 0\n",
    "        for image in image_files:\n",
    "            image_file = os.path.join(folder, image)\n",
    "            try:\n",
    "                image_data = (ndimage.imread(image_file).astype(float) - pixel_depth / 2) / pixel_depth\n",
    "                if image_data.shape != (image_size, image_size):\n",
    "                    raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "                dataset[num_images, :, :] = image_data\n",
    "                num_images = num_images + 1\n",
    "            except IOError as e:\n",
    "                print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "        \n",
    "        dataset = dataset[0:num_images, :, :]\n",
    "        if num_images < min_num_images:\n",
    "            raise Exception('Many fewer images than expected: %d < %d' % (num_images, min_num_images))\n",
    "    \n",
    "        print('Full dataset tensor:', dataset.shape)\n",
    "        print('Mean:', np.mean(dataset))\n",
    "        print('Standard deviation:', np.std(dataset))\n",
    "        return dataset\n",
    "        \n",
    "    def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "        dataset_names = []\n",
    "        for folder in data_folders:\n",
    "            set_filename = folder + '.pickle'\n",
    "            dataset_names.append(set_filename)\n",
    "            if os.path.exists(set_filename) and not force:\n",
    "                # You may override by setting force=True.\n",
    "                print('%s already present - Skipping pickling.' % set_filename)\n",
    "            else:\n",
    "                print('Pickling %s.' % set_filename)\n",
    "                dataset = load_letter(folder, min_num_images_per_class)\n",
    "                try:\n",
    "                    with open(set_filename, 'wb') as f:\n",
    "                        pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "                except Exception as e:\n",
    "                    print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "        return dataset_names\n",
    "\n",
    "    train_datasets = maybe_pickle(train_folders, 45000)\n",
    "    test_datasets = maybe_pickle(test_folders, 1800)\n",
    "\n",
    "\n",
    "    def make_arrays(nb_rows, img_size):\n",
    "        if nb_rows:\n",
    "            dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "            label = np.ndarray(nb_rows, dtype=np.int32)\n",
    "        else:\n",
    "            dataset, label = None, None\n",
    "        return dataset, label\n",
    "\n",
    "    def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "        num_classes = len(pickle_files)\n",
    "        valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "        train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "        vsize_per_class = valid_size // num_classes\n",
    "        tsize_per_class = train_size // num_classes\n",
    "    \n",
    "        start_v, start_t = 0, 0\n",
    "        end_v, end_t = vsize_per_class, tsize_per_class\n",
    "        end_l = vsize_per_class+tsize_per_class\n",
    "        for label, pickle_file in enumerate(pickle_files):       \n",
    "            try:\n",
    "                with open(pickle_file, 'rb') as f:\n",
    "                    letter_set = pickle.load(f)\n",
    "                    np.random.shuffle(letter_set)\n",
    "                    if valid_dataset is not None:\n",
    "                        valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "                        valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "                        valid_labels[start_v:end_v] = label\n",
    "                        start_v += vsize_per_class\n",
    "                        end_v += vsize_per_class\n",
    "                    \n",
    "                    train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "                    train_dataset[start_t:end_t, :, :] = train_letter\n",
    "                    train_labels[start_t:end_t] = label\n",
    "                    start_t += tsize_per_class\n",
    "                    end_t += tsize_per_class\n",
    "            except Exception as e:\n",
    "                print('Unable to process data from', pickle_file, ':', e)\n",
    "                raise\n",
    "    \n",
    "        return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "    train_size = 200000\n",
    "    valid_size = 10000\n",
    "    test_size = 10000\n",
    "\n",
    "    valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(train_datasets, train_size, valid_size)\n",
    "    _, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "    print('Training:', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Testing:', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "    def randomize(dataset, labels):\n",
    "        permutation = np.random.permutation(labels.shape[0])\n",
    "        shuffled_dataset = dataset[permutation,:,:]\n",
    "        shuffled_labels = labels[permutation]\n",
    "        return shuffled_dataset, shuffled_labels\n",
    "\n",
    "    train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "    test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "    valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)\n",
    "\n",
    "    pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "    try:\n",
    "        f = open(pickle_file, 'wb')\n",
    "        save = {\n",
    "        'train_dataset': train_dataset,\n",
    "        'train_labels': train_labels,\n",
    "        'valid_dataset': valid_dataset,\n",
    "        'valid_labels': valid_labels,\n",
    "        'test_dataset': test_dataset,\n",
    "        'test_labels': test_labels,\n",
    "        }\n",
    "        pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "        raise\n",
    "    \n",
    "    statinfo = os.stat(pickle_file)\n",
    "    print('Compressed pickle size:', statinfo.st_size)\n",
    "\n",
    "    \n",
    "    image_size = 28\n",
    "    num_labels = 10\n",
    "    \n",
    "    def reformat(dataset, lables):\n",
    "        dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "        lables = (np.arange(num_labels) == lables[:,None]).astype(np.float32)\n",
    "        return dataset, lables\n",
    "    \n",
    "    print('Training:', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Testing:', test_dataset.shape, test_labels.shape)\n",
    "    train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "    valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "    test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "    print('Training:', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Testing:', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    # Create a multilayer model.\n",
    "\n",
    "    # Input placeholders\n",
    "    with tf.name_scope('input'):\n",
    "        x = tf.placeholder(tf.float32, [None, 784], name='x_input')\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10], name='y_input')\n",
    "\n",
    "    with tf.name_scope('input_reshape'):\n",
    "        image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        tf.summary.image('image', image, 10)\n",
    "\n",
    "    # We can't initialize these variables to 0 - the network will get stuck.\n",
    "    def weight_variable(shape):\n",
    "        \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(shape):\n",
    "        \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def variable_summaries(var):\n",
    "        \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "    def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "        \"\"\"Reusable code for making a simple neural net layer.\n",
    "        It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
    "        It also sets up name scoping so that the resultant graph is easy to read,\n",
    "        and adds a number of summary ops.\n",
    "        \"\"\"\n",
    "        # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "        with tf.name_scope(layer_name):\n",
    "            # This Variable will hold the state of the weights for the layer\n",
    "            with tf.name_scope('weights'):\n",
    "                weights = weight_variable([input_dim, output_dim])\n",
    "                variable_summaries(weights)\n",
    "            with tf.name_scope('biases'):\n",
    "                biases = bias_variable([output_dim])\n",
    "                variable_summaries(biases)\n",
    "            with tf.name_scope('Wx_plus_b'):\n",
    "                preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "            tf.summary.histogram('pre_activations', preactivate)\n",
    "            activations = act(preactivate, name='activation')\n",
    "            tf.summary.histogram('activations', activations)\n",
    "            return activations\n",
    "\n",
    "    hidden1 = nn_layer(x, 784, 500, 'layer1')\n",
    "\n",
    "    with tf.name_scope('dropout'):\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        #tf.summary.scalar('druoput__keep_probability', keep_prob)\n",
    "        dropped = tf.nn.dropout(hidden1, keep_prob)\n",
    "\n",
    "    # Do not apply softmax activation yet, see below.\n",
    "    y = nn_layer(dropped, 500, 10, 'layer2', act=tf.identity)\n",
    "\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "        with tf.name_scope('total'):\n",
    "            cross_entropy = tf.reduce_mean(diff)\n",
    "        #tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer(FLAGS.learning__rate).minimize(cross_entropy)\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    #tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    # Merge all the summaries and write them out to /tmp/tensorflow/mnist/logs/mnist_with_summaries (by default)\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(FLAGS.log__dir + '/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(FLAGS.log__dir + '/test')\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    def get_batch_data(data,label,batch_size):\n",
    "        start_index = np.random.randint(0, len(data) - batch_size)\n",
    "        return data[start_index : start_index + batch_size], label[start_index : start_index + batch_size]\n",
    "          \n",
    "    # Train the model, and also write summaries.\n",
    "    # Every 10th step, measure test-set accuracy, and write test summaries\n",
    "    # All other steps, run train_step on training data, & add training summaries\n",
    "\n",
    "    def feed_dict(train):\n",
    "        \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "        if train or FLAGS.fake_data:\n",
    "            xs, ys = get_batch_data(data = train_dataset, label = train_labels, batch_size = 100)\n",
    "            k = FLAGS.dropout\n",
    "        else:\n",
    "            xs, ys = get_batch_data(data = test_dataset, label = test_labels, batch_size = 100)\n",
    "            #xs, ys = test_dataset, test_labels\n",
    "            k = 1.0\n",
    "        return {x: xs, y_: ys, keep_prob: k}\n",
    "\n",
    "    for i in range(FLAGS.max__steps):\n",
    "        if i % 20 == 0:  # Record summaries and test-set accuracy\n",
    "            summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "            test_writer.add_summary(summary, i)\n",
    "            print('Accuracy at step %s: %s' % (i, acc))\n",
    "        else:  # Record train set summaries, and train\n",
    "            if i % 10 == 0:  # Record execution stats\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run([merged, train_step],\n",
    "                                      feed_dict=feed_dict(True),\n",
    "                                      options=run_options,\n",
    "                                      run_metadata=run_metadata)\n",
    "                train_writer.add_run_metadata(run_metadata, 'step%03d' % i)\n",
    "                train_writer.add_summary(summary, i)\n",
    "                print('Adding run metadata for', i)\n",
    "            else:  # Record a summary\n",
    "                summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "                train_writer.add_summary(summary, i)\n",
    "                \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, FLAGS.log__dir, global_step = i)\n",
    "    \n",
    "    train_writer.close()\n",
    "    test_writer.close()\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    if tf.gfile.Exists(FLAGS.log__dir):\n",
    "        tf.gfile.DeleteRecursively(FLAGS.log__dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.log__dir)\n",
    "    train()\n",
    "\n",
    "          \n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import the related packages\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import ndimage\n",
    "from six.moves import xrange\n",
    "from six.moves import cPickle as pickle\n",
    "from IPython.display import display, Image\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "\n",
    "# define some funtion to sample the code\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "    \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "    slow internet connections. Reports every 5% change in download progress.\n",
    "    \"\"\"\n",
    "    global last_percent_reported\n",
    "    percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "    if last_percent_reported != percent:\n",
    "        if percent % 5 == 0:\n",
    "            sys.stdout.write(\"%s%%\" % percent)\n",
    "            sys.stdout.flush()\n",
    "        else:\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "    \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    dest_filename = os.path.join(data_root, filename)\n",
    "    if force or not os.path.exists(dest_filename):\n",
    "        print('Attempting to download:', filename) \n",
    "        filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "        print('\\nDownload Complete!')\n",
    "    statinfo = os.stat(dest_filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', dest_filename)\n",
    "    else:\n",
    "        raise Exception(\n",
    "      'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "    return dest_filename\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "    if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "        print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall(data_root)\n",
    "        tar.close()\n",
    "    data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "    if len(data_folders) != num_classes:\n",
    "        raise Exception( 'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "    print(data_folders)\n",
    "    return data_folders\n",
    "\n",
    "\n",
    "def make_arrays(nb_rows, img_size):\n",
    "    \"\"\"Change the data and lables to the arrays \"\"\"\n",
    "    if nb_rows:\n",
    "        dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "        label = np.ndarray(nb_rows, dtype=np.int32)\n",
    "    else:\n",
    "        dataset, label = None, None\n",
    "    return dataset, label\n",
    "    \n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "    num_classes = len(pickle_files)\n",
    "    valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "    train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "    vsize_per_class = valid_size // num_classes\n",
    "    tsize_per_class = train_size // num_classes\n",
    "    \n",
    "    start_v, start_t = 0, 0\n",
    "    end_v, end_t = vsize_per_class, tsize_per_class\n",
    "    end_l = vsize_per_class+tsize_per_class\n",
    "    for label, pickle_file in enumerate(pickle_files):       \n",
    "        try:\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                letter_set = pickle.load(f)\n",
    "                np.random.shuffle(letter_set)\n",
    "                if valid_dataset is not None:\n",
    "                    valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "                    valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "                    valid_labels[start_v:end_v] = label\n",
    "                    start_v += vsize_per_class\n",
    "                    end_v += vsize_per_class\n",
    "                    \n",
    "                train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "                train_dataset[start_t:end_t, :, :] = train_letter\n",
    "                train_labels[start_t:end_t] = label\n",
    "                start_t += tsize_per_class\n",
    "                end_t += tsize_per_class\n",
    "        except Exception as e:\n",
    "            print('Unable to process data from', pickle_file, ':', e)\n",
    "            raise\n",
    "    \n",
    "    return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "    \"\"\"Load the data for a single letter label.\"\"\"\n",
    "    image_files = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size),dtype=np.float32)\n",
    "    print(folder)\n",
    "    num_images = 0\n",
    "    for image in image_files:\n",
    "        image_file = os.path.join(folder, image)\n",
    "        try:\n",
    "            image_data = (ndimage.imread(image_file).astype(float) - pixel_depth / 2) / pixel_depth\n",
    "            if image_data.shape != (image_size, image_size):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "            dataset[num_images, :, :] = image_data\n",
    "            num_images = num_images + 1\n",
    "        except IOError as e:\n",
    "            print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "        \n",
    "    dataset = dataset[0:num_images, :, :]\n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('Many fewer images than expected: %d < %d' % (num_images, min_num_images))\n",
    "    \n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    print('Mean:', np.mean(dataset))\n",
    "    print('Standard deviation:', np.std(dataset))\n",
    "    return dataset\n",
    "\n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "    \"\"\"Check the pickle file and load the pictures\"\"\"\n",
    "    dataset_names = []\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder + '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        if os.path.exists(set_filename) and not force:\n",
    "            # You may override by setting force=True.\n",
    "            print('%s already present - Skipping pickling.' % set_filename)\n",
    "        else:\n",
    "            print('Pickling %s.' % set_filename)\n",
    "            dataset = load_letter(folder, min_num_images_per_class)\n",
    "            try:\n",
    "                with open(set_filename, 'wb') as f:\n",
    "                    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "    return dataset_names\n",
    "\n",
    "def save_pickle():\n",
    "    pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "    try:\n",
    "        f = open(pickle_file, 'wb')\n",
    "        save = {\n",
    "        'train_dataset': train_dataset,\n",
    "        'train_labels': train_labels,\n",
    "        'valid_dataset': valid_dataset,\n",
    "        'valid_labels': valid_labels,\n",
    "        'test_dataset': test_dataset,\n",
    "        'test_labels': test_labels,\n",
    "        }\n",
    "        pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "        raise\n",
    "    \n",
    "    statinfo = os.stat(pickle_file)\n",
    "    print('Compressed pickle size:', statinfo.st_size)\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "    \"\"\"Random the datas and the lables\"\"\"\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "    \n",
    "def reformat(dataset, lables):\n",
    "    \"\"\"Change the shape of the datasets and lables\"\"\"\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    lables = (np.arange(num_labels) == lables[:,None]).astype(np.float32)\n",
    "    return dataset, lables\n",
    "    \n",
    "# We can't initialize these variables to 0 - the network will get stuck.\n",
    "def weight_variable(shape):\n",
    "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)\n",
    "    \n",
    "def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "    \"\"\"Reusable code for making a simple neural net layer.\n",
    "    It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
    "    It also sets up name scoping so that the resultant graph is easy to read,\n",
    "    and adds a number of summary ops.\n",
    "    \"\"\"\n",
    "    # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "    with tf.name_scope(layer_name):\n",
    "        # This Variable will hold the state of the weights for the layer\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = weight_variable([input_dim, output_dim])\n",
    "            variable_summaries(weights)\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = bias_variable([output_dim])\n",
    "            variable_summaries(biases)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "        tf.summary.histogram('pre_activations', preactivate)\n",
    "        activations = act(preactivate, name='activation')\n",
    "        tf.summary.histogram('activations', activations)\n",
    "    return activations\n",
    " \n",
    "def get_batch_data(data,label,batch_size):\n",
    "    \"\"\"Get the batch datas and the lables\"\"\"\n",
    "    start_index = np.random.randint(0, len(data) - batch_size)\n",
    "    return data[start_index : start_index + batch_size], label[start_index : start_index + batch_size]\n",
    "    \n",
    "def feed_dict(train):\n",
    "    \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "    if train or FLAGS.fake_data:\n",
    "        xs, ys = get_batch_data(data = train_dataset, label = train_labels, batch_size = 100)\n",
    "        k = FLAGS.dropout\n",
    "    else:\n",
    "        xs, ys = get_batch_data(data = test_dataset, label = test_labels, batch_size = 100)\n",
    "        #xs, ys = test_dataset, test_labels\n",
    "        k = 1.0\n",
    "    return {x: xs, y_: ys, keep_prob: k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified .\\notMNIST_large.tar.gz\n",
      "Found and verified .\\notMNIST_small.tar.gz\n",
      ".\\notMNIST_large already present - Skipping extraction of .\\notMNIST_large.tar.gz.\n",
      "['.\\\\notMNIST_large\\\\A', '.\\\\notMNIST_large\\\\B', '.\\\\notMNIST_large\\\\C', '.\\\\notMNIST_large\\\\D', '.\\\\notMNIST_large\\\\E', '.\\\\notMNIST_large\\\\F', '.\\\\notMNIST_large\\\\G', '.\\\\notMNIST_large\\\\H', '.\\\\notMNIST_large\\\\I', '.\\\\notMNIST_large\\\\J']\n",
      ".\\notMNIST_small already present - Skipping extraction of .\\notMNIST_small.tar.gz.\n",
      "['.\\\\notMNIST_small\\\\A', '.\\\\notMNIST_small\\\\B', '.\\\\notMNIST_small\\\\C', '.\\\\notMNIST_small\\\\D', '.\\\\notMNIST_small\\\\E', '.\\\\notMNIST_small\\\\F', '.\\\\notMNIST_small\\\\G', '.\\\\notMNIST_small\\\\H', '.\\\\notMNIST_small\\\\I', '.\\\\notMNIST_small\\\\J']\n",
      ".\\notMNIST_large\\A.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\B.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\C.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\D.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\E.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\F.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\G.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\H.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\I.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\J.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\A.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\B.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\C.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\D.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\E.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\F.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\G.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\H.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\I.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\J.pickle already present - Skipping pickling.\n",
      "Compressed pickle size: 690800503\n",
      "Training: (200000, 784) (200000, 10)\n",
      "Validation: (10000, 784) (10000, 10)\n",
      "Testing: (10000, 784) (10000, 10)\n",
      "Accuracy at step 0: 0.12\n",
      "Accuracy at step 10: 0.73\n",
      "Accuracy at step 20: 0.86\n",
      "Accuracy at step 30: 0.87\n",
      "Accuracy at step 40: 0.88\n",
      "Accuracy at step 50: 0.88\n",
      "Accuracy at step 60: 0.9\n",
      "Accuracy at step 70: 0.91\n",
      "Accuracy at step 80: 0.89\n",
      "Accuracy at step 90: 0.91\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEZCAYAAACTsIJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XucHFWZ//HPN4SACYFAuAZIwn1BBQwYQQRGwCUoS366\n6ILiAq7KKggrKwu6ugzuRXFXXBZEXUVcUAnKiiArcp9wh0ACIZCQqCQEEkBCglwEcnl+f5zqpHrS\nPVNzqenume/79erXVFVXn3q6uqeernOqzlFEYGZmVjGs0QGYmVlzcWIwM7MqTgxmZlbFicHMzKo4\nMZiZWRUnBjMzq+LEYHVJmiBpjSR/T/qJpN0lzZL0kqTTCr5mjaSdy46tryQ9KemwAuv5e9Xk/MHY\nWnX+sX2jS//6B+C2iNgsIi7u/KSk2yV9otPiwfgZDMb3NGg4MdiQIGmDRseQmQA81sPXqIxAzOpx\nYjAAJF0OjAd+JemPkr5QeQo4QdIiSc9L+lLuNZJ0jqTfSvqDpGmSxtQpf4ykX2VlLMumx+We31zS\nDyU9kz3/i9xzU3PVLwsk/Xm2vOoMR9K5kq7IpivVFZ+QtAi4NVv+M0lLJS2X1CFpr9zrN5b0TUkL\nJa2QdEe27HpJp3Z6P49ImlrnvR4jaY6kFyXdJmmPbPmtwHuBb2f7eNdOr/sX4GDg4uz5/8o9/T5J\n87MyL+70uk9IejzbbzdIGl8nrso+OUnSU9n6p0jaP3s/L0q6KLe+JH052x/PSvqRpE1zz388e+4P\n+e9F7rWFvhvWhCLCDz+ICIAngffm5icAa4DvASOAvYHXgT2y588A7gG2AzYEvgP8tE7ZWwAfBDYC\nRgFXAdfknv8/4EpgU2AD4OBs+WRgBXBYNr8dsHsu3sNyZZwLXN4p9h8BbwE2ypafBIzM4r0AmJV7\n/beB24BtSQnxgGy9DwP35dbbB/gDMLzG+9wdeAU4LHsfZwELKusCtwOf6OIzWO/57H1cB4wGdgSe\nB/48e24qMD/b7jDgS8Dddcqu7JNLss/zCOBPwC+AscA44Lncvv9EVvaEbJ/9b27/7gW8DByU7aNv\nAm/mPqe6342svNXAsEZ/5/2o8z1sdAB+NM+jxoG28g+8XW7Z/cBHsunHqU4k22UHh27/4YF9gWW5\n160CNq2x3neBbxaMt3NiWA1M6CKGMdmBcnSWCF4D3lZjvY2AZcAu2fy/AxfXKfPLwLTcvICngUOy\n+d4mhgNz81cB/5BN/xo4OffcMOBVYMcaZVf2yba5ZS8AH87NXw2cnk3fAvxt7rndgTeybXyF3I+A\nLHG8kUsMdb8bTgzN/3BVkhXxXG76NWCTbHoCcE1WBfEi6WCwEtimcwGS3iLpe5VqGmA6MEaSgB2A\nFyPijzW2vSPwuz7E/nQuhmGSvp5Vb6wgJZYAtsweGwG/71xARLxBOhifkMV7PHBFne2NAxblXhvA\nYmD7PrwH6PozuDD3GSwjvaeutvd8bvpPncr+U67sqveSTQ8nfb7jSO8LgIh4Ldt2ReHvhjUfJwbL\n6+mVIk8BR0XEFtlj84gYFRFLa6z798BuwDsjYgxwSLZcpAPMFvn665zFwC51tv8q6ZdqxbY11sm/\np48Cf0H6VTsGmJhtX6Rfzq93sa3LgROAw4FXI+L+OustIR0U83Ykl6C60dPPYDFwSqfPYJOIuK+H\n5dTS+b1MIJ3ZPQcsJb0vACSNJFVHVfTku2FNxonB8p4FOl8v39UVMd8D/q3S2ClpK0nH1Fl3NOnX\n6B8lbQG0V56IiGeBG4BLskbq4ZIOzp6+FDhZ0nuzBs1xlcZc4GHguGz9/YFju4l9NKm6Y7mkUcDX\nyA7E2S/7y4ALJG2XnV0cIGnD7Pn7SFU636T+2QLAz4APZPEOV2rEfx24t4vX5D3H+p9BV74LfKnS\niC5pM0md90NeT65wuhL4vKSJkjYB/pVUTbaGVOV0tKR3Z/voq53K7u674SutmpgTg+V9HfhKdvp/\nZras8y/Y/PyFwLXATZJeIjU2Tq5T9n+Sft2/kK33607Pf5z0a3Qe6eB4BkBEzABOzl7/EtBBunoK\nUj33rsCLpPaFn3QRK6Rf/U8BzwBzsjjyvgA8CswgVYt8ner/kcuBtwE/rvMeiYj5pDOLi0kN1B8A\n/iIiVtWJqbMLgQ9nVwz9Z53XrJ2PiF9mcU7LqsdmA1O6KL+rz7Pz/A9JSfAOUnXea8Dp2XYfB04l\nJY8lpP2VPyvq7rvh+xiamNIPpZIKly4Fjgaei4i966zzX8BRpGqBkyLi4dICMusDSR8HPhURh3S7\nslkLK/uM4TLgyHpPSjqKdKXHbsAppNNis6aT1aF/llRFYjaolZoYIuIuYHkXq0wlnZ6TNeZtJslX\nLVhTUbqh7nlSg+uVDQ7HrHTDG7z97cld8kaq+92e6svnzBoqIm5i3SWcZoOeG5/NzKxKo88YniF3\nLTTpRqdnaq0oyVcxmJn1QkT06PLggThjqNxAVMt1wF8DSDoAWBERdauRGn2beOfHueee2/AYWiGm\nZo3LMTmmoRBXb5R6xiDpp0AbMFbSU6RrzUeQ7if674j4taT3S/ot6XLVk8uMx8zMuldqYoiIjxZY\np9AoVmZmNjDc+NwHbW1tjQ5hPc0YEzRnXI6pGMdUXLPG1VOl3vncnyRFq8RqZtYsJBFN2PhsZmYt\nxInBzMyqODGYmVkVJwYzM6vixGBmZlWcGMzMrIoTg5mZVXFiMDOzKk4MZmZWxYnBzMyqODGYmVkV\nJwYzM6vixGBmZlWcGMzMrErpiUHSFEnzJM2XdHaN58dLukXSI5JukzSu7JjMzKy+sof2HAZcDBwO\nLAFmSLo2IublVvsP4EcR8WNJbcDXycaBNrOhJwJeeQVefBGWL6/9eO012GEH2HnndY/RoxsdeeOt\nXAlPPQW///26R2+UmhiAycCCiFgEIGkaMBXIJ4a9gM8DRESHpGtLjsnMShYBL79c/8De1WPFCtho\nI9h88+rHFltUTz/9NNxxx7oD4KhR1Yki/9h+e9hgg0bvlb6LSPsof+D/3e/WTS9ZAuPGVb/33ih1\nBDdJfwkcGRGfzuZPACZHxOm5dX4M3B8RF0n6EPBzYMuIWN6pLI/gZjbAVq6EpUu7/vXek4N75wN8\nrceYMTBiRM/ijIDnnqs+YOYPnMuWwYQJ9RNHM51t1PrVnz/4R8Auu1THX5kfPx423LC6vN6M4Fb2\nGUMRZwEXSzoJuAN4Blhda8X29va1021tbYNmfFWzRqn1CzR/IFqyBLbaqv7B/K1v7b+De19IsO22\n6fHud6///J/+BAsXVr/H/NnGyJHrH2zLOtvoza/+D394XXybb57ebz0dHR10dHT0KcayzxgOANoj\nYko2fw4QEXF+nfVHAXMjYnyN53zGYNYLvfkFWnmMHz+wB/hGKHK2MX58/X1U62yjt/t8l11q/+rv\ni96cMZSdGDYAniA1Pi8FHgCOj4i5uXXGAi9GREj6F2BVRLTXKMuJwayG3vwCzT+22KLrX6BDXa2z\njfxj5Mh1SfTFF7vf57vs0v2v/v7UdIkB0uWqwIWkS2MvjYivSzoPmBER12ftEF8D1pCqkk6NiJU1\nynFisCHLv/qbU/5sY9EiGDs27fMJE/r3V39fNGVi6C9ODNYfKlfLrFgBb77Z6GjWt2KFf/Vb/3Ji\nsCGhJ5dCdr6aZsUK2HjjdCq/0UaNfifrGz269i//ZvoFaq3FicFaRtGDe63LJPMH955cBtmIq2XM\nGs2JwQZUGQf3Igd5H9zNinNisB4r8+De1YHeB3ezgeHEYMyfD88807s6dx/czQYfJ4YhLALOOw8u\nuQT23NMHdzNLWrVLDOujN9+ET34S5s2DRx+FbbZpdERm1so8UE+LW74cjjwytRN0dDgpmFnfOTG0\nsIUL4aCDYN994eqr0635ZmZ95cTQoh54IPUi+ZnPwLe+NTj6mjez5uA2hhb0y1/Cpz4Fl14KxxzT\n6GjMbLBxYmgxF14I3/gG3HAD7L9/o6Mxs8HIiaFFrF4NZ54Jt9wCd98NEyc2OiIzG6ycGFrAq6/C\nRz+aBki/++5074GZWVnc+Nzknn0W2trSzWg33OCkYGblK5QYJG0gaZyk8ZVH2YEZPP44HHhgamC+\n7DLfoWxmA6PbxCDpc8BzwM3A/2WP64tuQNIUSfMkzZd0do3nd5R0m6SZkh6WdFQP4h+0brstnSl8\n9avwla94EBYzGzjd9pUk6bfAuyJiWY8Ll4YB80ljPi8BZgDHRcS83DrfA2ZGxPck7Qn8OiJ2qlHW\nkOkr6fLL4ayz4KqrUnIwM+utsvpKWgy81LuQmAwsiIhFAJKmAVOBebl11gCbZtNjgGd6ua2WV+kI\n7/LLU/cWe+7Z6IjMbCiqmxgknZlN/h7okPR/wBuV5yPiggLlb09KLBVPk5JF3nnATZJOB0YCRxQo\nd9DJd4R3773u88jMGqerM4bR2d+nsseI7NHfjgcui4hvSToA+DHw1lortre3r51ua2ujbZDUsyxf\nDn/5l7DZZulMwX0emVlvdXR00NHR0acySh2PITvQt0fElGz+HCAi4vzcOnOAIyPimWz+d6Q2jRc6\nlTUo2xgWLoT3vz/1kPof/+E+j8ysf/WmjaHIVUk3SxqTm99c0o0Fy58B7CppgqQRwHHAdZ3WWURW\nfZQ1Pm/UOSkMVjNmuCM8M2s+RRqft4qIFZWZiFguaesihUfEakmnATeRktClETFX0nnAjIi4HvgC\n8H1Jnyc1RJ/Y43fRgq69NnWE94MfuCM8M2suRS5XfQj4YEQ8lc1PAK6JiEkDEF8+jkFTlVTpCO/a\na90RnpmVq6zLVf8RuEvSdEDAwcCnexHfkJfvCO+ee2DChEZHZGa2vkKNz5K2BA7IZu9rRBtAq58x\nvPoqfOxjaQjO//1f93lkZgOjlMbnzLuBtuxxQJdr2nrcEZ6ZtZIiVyV9HTgDeDx7nCHp38oObLDI\nd4T3wx+6Izwza35FGp9nA/tGxJpsfgNgVkTsPQDx5eNouaqk226D449P9yd8/OONjsbMhqIyq5Ly\nlR+b9WQDQ9Xll6ekcNVVTgpm1lqKXJX0NWCWpNtJVyUdApxTalQtLCJ1lf0//+OO8MysNRW9Kmk7\n4J1AkG5Me7bswGrE0PRVSW++mW5amzcPrrvOHeGZWeOVdR8DwIHAe0iJYThwTQ9jG/TyHeHdfrs7\nwjOz1lXkqqRLgL8FHgXmAKdI+nbZgbWShQvhoINgn33g6qudFMystRW5Kukx4G2VepxsVLZHI6Jm\n19hladaqpBkzYOpU+OIX4XOfa3Q0ZmbVyroq6QlgfG5+R2B2TzYyWF17LXzgA/Dd7zopmNngUeSM\nYTqp4fmBbNE7gXuB1wAiYkD6Bm22M4aZM9M4Ctdf747wzKx5ldX4/E+9jGdQ+81v0n0KTgpmNth0\nmxgiYnrW1fZuEXGLpLcAwyPi5fLDa17Tp6cBdszMBpsiVUmfInWzvUVE7CJpN+C7EXH4QASYi6Np\nqpJWroSxY+HJJ9NfM7NmVVbj86nAQcAfASJiAVBoBLcsqCmS5kmaL+nsGs9fIGmWpJmSnpD0YtGy\nG2XWLJg40UnBzAanIm0Mb0TEm1JKOJKGk25061Z2aevFwOHAEmCGpGsjYl5lnYg4M7f+acC+xcNv\njOnT4dBDGx2FmVk5ipwxTJf0JeAtkt4H/Bz4VcHyJwMLImJRRKwEpgFTu1j/eODKgmU3jBODmQ1m\nRRLDOcAfSHc+nwL8GvhywfK3Bxbn5p/Olq1H0nhgInBbwbIbYvVquOsuOOSQRkdiZlaOIlclrQG+\nnz3KdBxwdVctzO3t7Wun29raaGtrKzmk9T3yCIwbB1sXbmUxMxs4HR0ddHR09KmMQr2r9rpw6QCg\nPSKmZPPnABER59dYdybw2Yi4r05ZTXFV0re+BfPnw3e+0+hIzMy6V+ZAPb01A9hV0gRJI0hnBdd1\nXknSnwFj6iWFZuL2BTMb7EpNDBGxGjgNuAl4DJgWEXMlnSfp6Nyqf0VqmG5qa9bAnXc6MZjZ4Fbk\nBrfdgbOACeTaJCLisHJDWy+OhlclzZ4Nxx6bqpLMzFpBWX0l/Rz4LqnxeXVvAhsspk/31UhmNvgV\nSQyrIsJNraTEMLWruzDMzAaBulVJkrbIJk8HnicN5/lG5fmIGNCuKxpdlRSRxnB+8EEYP7779c3M\nmkF/VyU9ROr6olLgWbnnAti5Z+G1trlzYdQoJwUzG/zqJoaI2GkgA2l2vkzVzIaKbi9XlXSqpDG5\n+c0lfbbcsJqPE4OZDRVFLld9OCL27bRsVkS8o9TI1o+jYW0MEakbjLvvhp2HVAWambW6su583kCV\nPrfTRjYARvQ0uFa2YAEMHw47uXLNzIaAIper/ga4StL3svlTsmVDRqUaST3KuWZmralIYjiblAwq\nIxzfDPygtIiakNsXzGwoKbV31f7UqDaGiHSJ6q23wu67D/jmzcz6pJQuMSTtBnwN2AvYuLI8IoZE\nM+zChbBqFey2W6MjMTMbGEUany8DvgOsAt4LXA5cUWZQzcTtC2Y21BRJDG+JiFtJ1U6LIqIdGNCe\nVRvJ7QtmNtQUSQxvSBoGLJB0mqQPAkNmYEsnBjMbaookhjOAkaTO9PYDTgBOLLoBSVMkzZM0X9LZ\nddb5iKTHJD0q6cdFyy7b4sXwyiuw556NjsTMbOB02/gcETMAJK2JiJN7Unh2pnExcDiwBJgh6dqI\nmJdbZ1fSJbEHRsQfJW3Zk22UqTL+gtsXzGwoKdJX0oGSHgfmZfP7SLqkYPmTgQVZ28RK0vCdnUc0\n+BTw7Yj4I0BEvFA4+pK5GsnMhqIiVUn/CRwJLAOIiEeAouOYbQ8szs0/nS3L2x3YQ9Jdku6RdGTB\nskvnxGBmQ1GRO5+JiMWqrk/pzyE+hwO7kpLNeOAOSW+rnEE0ytKlsGwZvO1tjYzCzGzgFUkMiyW9\nGwhJG5Iao+cWLP8Z0sG+YodsWd7TwH0RsQZYKGk+sBtpoKAq7e3ta6fb2tpoa2srGEbPTZ8O73kP\nDCtyTmVm1iQ6Ojro6OjoUxlFut3eErgQOII0mttNwBkRsazbwlNPrE+QGp+XAg8Ax0fE3Nw6R2bL\nTsq29RCwb0Qs71TWgHaJ8ZnPpLudzzxzwDZpZtbvSukSI2sM/lhvAoqI1ZJOIyWTYcClETFX0nnA\njIi4PiJulPTnkh4j3V39hc5JoRGmT4dPfrLRUZiZDTx3olfD88+nDvOWLYMNNhiQTZqZlaKsgXqG\nnDvugIMOclIws6GpbmKQdEb296CBC6c5+DJVMxvKujpjqNzlfNFABNJMnBjMbCjrqvF5rqSFwFaS\nZueWC4iI2LvUyBpk2bI0BsOkSY2OxMysMeomhog4XtK2wI3AMQMXUmPdeScceCBsuGGjIzEza4wu\nL1eNiGeBfSSNIHVdAfBE1u/RoORqJDMb6op0oncosAD4NnAJMF9S0b6SWo4Tg5kNdUXufH4I+GhE\nPJHN7w5cGRH7DUB8+ThKv4/hpZdghx1SO8OIEaVuysxsQJR1H8OGlaQAEBHzgUFZA3/XXTB5spOC\nmQ1tRTrRe1DSD4DKyGofAx4sL6TGcTWSmVmxM4bPAI+ThvY8PZv+TJlBNYoTg5mZ+0pa6+WXYbvt\n4IUXYOONS9uMmdmAcl9JfXDPPbDffk4KZmZODBlXI5mZJUXuY3j7QATSaE4MZmZJkfsY7gQ2An4E\n/CQiXhqAuGrFUVobw2uvwdZbp3EYRo4sZRNmZg1RShtDRBxMukR1R+AhST+V9L4eBDVF0jxJ8yWd\nXeP5EyU9L2lm9vhET95Af7j3Xth7bycFMzModh8DEbFA0pdJ9y/8F/AOSQK+FBG/qPc6ScOAi0lj\nPi8BZki6NiLmdVp1WkSc3qt30A9cjWRmtk6RNoa9JX0LmAscBvxFROyZTX+rm5dPBhZExKKs471p\nwNRam+lZ2P3LicHMbJ0iVyVdBMwE9omIUyNiJkBELAG+3M1rtwcW5+afzpZ19iFJD0v6maQdCsTU\nb15/HR56KA3laWZmxRLDB4CfRsSfIFUPSRoJEBFX9EMM1wETI2Jf4Bbgf/qhzMLuvx/22gtGjx7I\nrZqZNa8ibQy3AEcAr2TzI4GbgHcXeO0zwPjc/A7ZsrUiYnlu9gfAN+oV1t7evna6ra2Ntra2AiF0\nzdVIZjaYdHR00NHR0acyilyu+nD2a77LZXVeuwHwBKnxeSnwAHB8RMzNrbNtNiAQkj4InBUR6yWd\nsi5XPfxw+Pzn4eij+71oM7OG683lqkXOGF6VNKnStiBpP+BPRQqPiNWSTiOdYQwDLo2IuZLOA2ZE\nxPXA6ZKOAVYCLwIn9eQN9MWbb8IDD8B73jNQWzQza35FzhjeSbqaaAnp6qFtgb+KiIfKD68qjn4/\nY7j7bvjc52DmzH4t1sysaZRyxhARMyT9GbBHtmjQjPns9gUzs/UV7URvD2AvYBJwvKS/Li+kgePE\nYGa2viJVSecCbaTE8GvgKOCuiDi29Oiq4+jXqqSVK2HsWHjyyfTXzGwwKms8hmNJVxU9GxEnA/uQ\nOtVrabNmwcSJTgpmZp0VSQx/iog1wCpJmwLPAzuXG1b5XI1kZlZbkcTwoKQxwPeBh0jdYzxQalQD\nwInBzKy2LtsYsh5Ud4iIxdn8RGDTiJg9INFVx9JvbQyrV6cqpPnz0zgMZmaDVb+3MWRH4l/m5hc2\nIin0t0cegXHjnBTMzGopUpV0X3aT26DhaiQzs/qKJIb3AvdK+p2k2ZIeldTSZw1ODGZm9RW5j2FC\nreURsaiUiOrH0S9tDGvWwFZbwaOPpuokM7PBrKxO9Pq/S9MGmjMHttjCScHMrJ4iieH/SMlBwMbA\nTqSutN9aYlylcTWSmVnXinSi9/b8vKRJwCmlRVSy6dNhaq1Rp83MDCjQxlDzRdJDEbFfCfF0tc0+\ntzFEwDbbwIMPwvjx3a9vZtbqSmljkHRmbnYYqYfVF3oYW1OYOxdGjXJSMDPrSpHLVUfnHhuR2hwK\nV8ZImiJpnqT5ks7uYr2/lLQmq6oqhdsXzMy6V6SN4bzeFi5pGHAxqXfWJcAMSddGxLxO620CnA7c\n19ttFTF9Ohx5ZJlbMDNrfd2eMUi6OetErzK/uaQbC5Y/GVgQEYuyUd+mUfts45+BrwNvFCy3xyJ8\nxmBmVkSRqqStImJFZSYilgNFexnaHlicm386W7aWpHeQOuq7oWCZvbJgAQwfDjvtVOZWzMxaX5H7\nGFZLGh8RT8HaO6H75aa3rPfWC4AT84vrrd/e3r52uq2tjba2tsLbqpwtqEdt82ZmraWjo4OOjo4+\nlVGkS4wpwH8D00kH7YOBT0dEt9VJkg4A2iNiSjZ/DqnT1vOz+U2B3wKvZGVvCywDjomImZ3K6tPl\nqieckBLDpz7V6yLMzFpOby5XLXQfg6QtgQOy2fsiotDlqpI2IN0lfTiwlDTAz/ERMbfO+rcDZ0bE\nrBrP9ToxRKRLVG+9FXbfvVdFmJm1pFLGfJb0QWBlRFwfEdeThvj8f0UKj4jVwGnATcBjwLSImCvp\nPElH13oJXVQl9dbChbBqFey2W3+XbGY2+BSpSno4IvbttGxWRLyj1MjWj6PXZww/+hH85jcwbVr/\nxmRm1uxKOWOos06RRuum4ctUzcyKK5IYHpR0gaRdsscFwENlB9afnBjMzIorkhg+B7wJXJU93gBO\nLTOo/rR4MbzyCuy5Z6MjMTNrDUW6xHgVOGcAYinF9OlwyCG+f8HMrKgivatuBfwDaWCejSvLI+Kw\nEuPqN65GMjPrmSJVST8B5pFGbjsPWAjMKDGmflU5YzAzs2KKJIaxEXEp6V6G6RHxCdbd7NbUli6F\nF16At7+9+3XNzCwpctnpyuzvUkkfIHWfvUN5IfWf6dPh4INhWJH0Z2ZmQLHE8C+SNgP+HrgI2BT4\nfKlR9RO3L5iZ9VyvxnxuhN7c+bzXXnDFFbDfgI5ObWbWPErrRK8Z9DQxPP986jBv2TLYYIMSAzMz\na2JldYnRku64Aw46yEnBzKynBm1icPuCmVnvFOl2extJl0q6IZvfS9LflB9a3zgxmJn1TpEzhh8B\nNwLjsvn5wN+VFVB/WLYsjcEwaVKjIzEzaz1FEsOWEfEzYA1ARKwCVpcaVR/deScceCBsuGGjIzEz\naz1FEsOrksaSRlerjOP8UtENSJoiaZ6k+ZLOrvH8KZJmS5ol6Q5Jf1Y4+jpcjWRm1ntFRnCbRLqx\n7W3AHGAr4NiImN1t4dIwUtXT4aQ7pmcAx0XEvNw6m0TEK9n0XwCfjYijapRV+HLVSZPgoovSVUlm\nZkNZby5XLdLt9kxJhwJ7kMZjfiIiVnbzsorJwIKIWJQFOA2YSuqUr1L+K7n1NyGrsuqtl16CBQvg\nne/sSylmZkNX0SE6JwMTs/UnZRno8gKv2x5YnJt/OiuriqTPAmcCGwJ96s77rrtg8mQYMaIvpZiZ\nDV1FxmO4AtgFeJh1jc4BFEkMhUTEJcAlko4DvgKcVGu99vb2tdNtbW20tbWtt47bF8xsKOvo6KCj\no6NPZRRpY5gL7NXjjopY21DdHhFTsvlzgIiI8+usL2B5RIyp8VyhEN71LvjGN5wczMygvC4x5gDb\n9i4kZgC7SpogaQRwHHBdfgVJu+ZmjyY1VvfKyy/DY4+l5GBmZr1TtypJ0q9IVUajgcclPQC8UXk+\nIo7prvCIWC3pNOAmUhK6NCLmSjoPmBER1wOnSToCeBNYDpzY2zdzzz2pJ9WNN+5+XTMzq61uVVJ2\nJVJdETG9lIjqKFKV9KUvwfDh8NWvDlBQZmZNrl+rkrJhPKcD769M55f1NdgyeHxnM7O+K9L4PDMi\nJnVaNjsi9i41svXj6PKM4bXXYKut0jgMo0YNYGBmZk2sX29wk/QZ4LPAzpLydzmPBu7uXYjlufde\n2GcfJwUzs77q6j6GnwI3AF8DzsktfzkiXiw1ql7w/QtmZv2jbmKIiJdIneUdP3Dh9N706fDFLzY6\nCjOz1jcoxnx+/XXYcktYuhRGjx7gwMzMmtiQHfP5/vthr72cFMzM+sOgSAxuXzAz6z9ODGZmVqXl\n2xjefBPFEG8LAAAMcklEQVTGjoXFi2HMel3vmZkNbUOyjWHGDNhtNycFM7P+0vKJwdVIZmb9y4nB\nzMyqtHQbw6pVsMUW8OSTqZ3BzMyqDbk2hpkzYeJEJwUzs/7U0onB1UhmZv2v9MQgaYqkeZLmSzq7\nxvOfl/SYpIcl3Sxpx6JlOzGYmfW/UtsYJA0jjeF8OLCENAb0cRExL7fOocD9EfG6pL8F2iLiuBpl\nVbUxrF6dqpDmz4etty7tLZiZtbRmbGOYDCyIiEURsRKYBkzNr5CNCvd6NnsfsH2Rgh95BMaNc1Iw\nM+tvZSeG7YHFufmn6frA/zekMSC65WokM7NydDVQz4CSdAKwH1D3cN/e3r52+pZb2jj11LbS4zIz\nayUdHR10dHT0qYyy2xgOANojYko2fw4QEXF+p/WOAC4EDomIZXXKWtvGsGZNGt/50UdTdZKZmdXW\njG0MM4BdJU2QNAI4Drguv4KkdwDfBY6plxQ6mzMn3djmpGBm1v9KTQwRsRo4DbgJeAyYFhFzJZ0n\n6ehstW8Ao4CfS5ol6Zfdlev2BTOz8rRklxjHHgtTp8LHP97goMzMmlxvqpJaLjFEwDbbwIMPwvjx\njY7KzKy5NWMbQ7+bOxdGjXJSMDMrS8slBrcvmJmVy4nBzMyqtFRiiHBiMDMrW0slhgULYPhw2Gmn\nRkdiZjZ4tVRiqJwtqEft62Zm1hMtmRjMzKw8TgxmZlalpRLDqlWw226NjsLMbHBrqcTg9gUzs/K1\nXGIwM7NyOTGYmVmVlupEb82acFWSmVkPDPpO9JwUzMzKV3pikDRF0jxJ8yWdXeP5gyU9JGmlpA+V\nHY+ZmXWt1MQgaRhwMXAk8FbgeEl/1mm1RcCJwE/KjKUMfR1wuwzNGBM0Z1yOqRjHVFyzxtVTZZ8x\nTAYWRMSiiFgJTAOm5leIiKciYg7QGo0dOc34JWjGmKA543JMxTim4po1rp4qOzFsDyzOzT+dLTMz\nsybVUo3PZmZWvlIvV5V0ANAeEVOy+XOAiIjza6x7GfCriPhFnbJarqrJzKwZ9PRy1eFlBZKZAewq\naQKwFDgOOL6L9esG39M3ZmZmvVNqVVJErAZOA24CHgOmRcRcSedJOhpA0v6SFgPHAt+V9GiZMZmZ\nWdda5s5nMzMbGC3R+NzdTXIDFMOlkp6TNDu3bHNJN0l6QtKNkjYb4Jh2kHSbpMckPSrp9EbHJWkj\nSfdLmpXFdG62fKKk+7LP8EpJZVdj1optmKSZkq5ropgWSnok218PZMsa/b3aTNLPJc3NvlvvavB3\navds/8zM/r4k6fQm2E+flzRH0mxJP5E0otHfKUlnZP93fToeNH1iKHiT3EC4LIsh7xzglojYA7gN\n+OIAx7QKODMi3gocCJya7ZuGxRURbwDvjYh3APsCR0l6F3A+8M2I2B1YAfzNQMWUcwbweG6+GWJa\nA7RFxDsiYnK2rNHfqwuBX0fEnsA+wLxGxhQR87P9MwnYD3gVuKaRMUkaB3wOmBQRe5Paa4+ngd8p\nSW/Ntrc/6X/vaEm70Jv9FBFN/QAOAG7IzZ8DnN2gWCYAs3Pz84BtsultgXkN3le/BI5olriAkcCD\npBsdnweG5T7T3wxwLDsANwNtwHXZsj80MqZsu08CYzsta9jnB2wK/K7G8mb5Tv05cGejYwLGkXpt\n2JyUFK4D3tfI7zmpnfb7ufkvA2cBc3u6n5r+jIHmvklu64h4DiAingW2blQgkiaSfiXcR/oSNCyu\nrMpmFvAs6WD8O2BFRKzJVnma9I81kL5F+ieJLMaxwPIGx0QWz42SZkj6ZLaskZ/fTsALki7Lqm7+\nW9LIBseU91fAT7PphsUUEUuAbwJPAc8ALwEzaez3fA5wcFZ1NBJ4P7AjvdhPrZAYWklDWvIlbQJc\nDZwREa/UiGNA44qINZGqknYgnS00oupvLUkfAJ6LiIepviS6GS6BPigi9if9E58q6WAa+/kNByYB\n345UdfMq6Sy9od8pAEkbAscAP68Tw4DFJGkMqXufCaSD/yhgykBtv5aImEeqyroZ+DUwC1hda9Xu\nymqFxPAMMD43v0O2rBk8J2kbAEnbkk4jB1TWuHU1cEVEXNsscQFExB+BDlL7x5isvQgG/jM8CDhG\n0u+BK4HDSPXomzUwJgAiYmn29w+kqsDJNPbzexpYHBEPZvP/S0oUzfCdOgp4KCJeyOYbGdMRwO8j\n4sVIl+VfQ/qeNfJ7TkRcFhH7R0QbqY3jCXqxn1ohMay9SU7SCNJNctc1KBZR/SvzOuCkbPpE4NrO\nLxgAPwQej4gLc8saFpekLStXPUh6C6ne9XHgduDDjYgpIr4UEeMjYmfS9+e2iDihkTEBSBqZne0h\naRSp/vxRGvj5ZVUOiyXtni06nHQPUjN8148nJfaKRsb0FHCApI0liXX7qdHfqa2yv+OBD5Kq3Xq+\nnwaqYaSPjSpTSJlvAXBOg2L4KbAEeIP0pTiZ1PB0SxbbTcCYAY7pINKp4sOk08aZ2b7aolFxAW/P\n4ngYmA38Y7Z8J+B+YD5wFbBhgz7HQ1nX+NzQmLLtVz67Ryvf7UZ+ftn29yH9IHsY+AWwWRPENJJ0\nscDo3LJGx3QuqWF3NvA/wIZN8J26g9TWMIt0tVuv9pNvcDMzsyqtUJVkZmYDyInBzMyqODGYmVkV\nJwYzM6vixGBmZlWcGMzMrIoTg/VJ1kXzZ3Lzh0r6VSNjqmWg4spu8LtP0kOSDiqh/AlqgsGsJN0u\naVKj47ByODFYX20OfLbTsma9OabXceW6OejOEcDciNgvIu7u7fa60az71wYJJwbrq68BO2c9cZ6f\nLRudG+jlisqKkiZJ6sh6Er2h0n9LXtar54WS7pb0W0kfypZX/eKXdJGkv86mn5T0r5LukfSApHdI\n+o2kBZI+nSt+M0m/UBp85pJcWe/LXvugpKuynikr5X5F0h2kLo3zcU6QdKvSIDs3Kw2atA+pE7P3\nZ/tjo06vqfn+JX0yi3tWtt82zpZvncX7cPbcAVlRw7NeT+dk77NqO9lrt5R0tdKgSfdLOjBbfq6k\ny7PYn9C6Hl2R9O9KA7w8IukjueVnKw1GM0vSv+U285Gs7HllnB1ZAw3k7dp+DL4H649RcSiwHNiO\n1K/UPcC7Sb123k029gDwEeDSGuVdBlyVTe8JLMiVe11uvYuAv86mnwQ+nU1fQOrKYSSwJfBs7vWv\nZfGK1DXAh4CxwHTgLdl6/wB8OVfuF+q87+uAE7Lpk4FrsukTgf+qsX7d9w9snlvvn4FTs+lpwOnZ\ntIDRWfwrgbdny68CPlpjez8B3p1N70jqTwtSNw6zgBHZe3+K1Ef/h4Abs3W2Jo01sA2pi5W7gI2y\n58Zkf28H/j2bPgq4udHfRT/67zHgQxnakPBAZL2GSnoYmEjqr/5twM1Zp2PDSH1P1fJLgIiYK6lo\nH/uVs4lHgVER8RrwmqTXJW2ai2tRFteVwHtIfV/tBdydxbUhKZlVXFVneweSOikDuIJ0ptCVPaj/\n/veW9M/AGFL3zTdmyw8DPg4Q6Qj8sqQtSL16VtoZHiLt386OAPbMtgWwSeVMCLg2It4Elkm6DXgX\naV9cmW3reUkdpJ5eDwUuizQyHxGxIreNX+RimNDN+7cW4sRgZXgjN72a9D0TMCciilQ55F9fObCt\norrqc+M6r1nT6fVrWPc9r9V/v4CbIuJjdWJ5tc7yntbzd/X+LwOOiYg5kk4kHYy72kbn/dt5X1S2\n966IWFm1MOWJ6LTeGtanLrbfOY7KZ2yDhNsYrK9eJlVxdOcJYKtKPbmk4ZL2KvC6SmJYBOwlaUOl\nQVIOLxhfvpv0d2VtA8NII4HdRRrx7iClsXErXWHvVqDce0jdQAOcANzZzfpdvf9NgGeVBqLJJ6hb\nyRr2lUbFq5z5FBlg6CbSGNdkr98n99xUpYHrx5KS0Iws/r/KtrMVcDDwAGnQl5OVulBH0uZ1ttcM\ngx5ZP3FisD6JiBdJ1TCzta7xuWqVbL2VpAbc87PqpVmk6pia69d4/dPAz1jXxfHMLl5Tr7x7gK+T\nqpt+FxHXRBr05STgSkmPZOvsUaDc00kHzIdJB/Mzuli3u/f/T6SD8I2kbpwr/g54r6TZpLGz9ywQ\nV8UZwP5ZQ/Ic4JTccw+QRvi6B/hqRDwbEdeQ9u0jpC6az4qI5yPiRlJ7yoOSZgJ/XycGXyk1iLjb\nbbMhRNK5wMsRcUGjY7Hm5TMGMzOr4jMGMzOr4jMGMzOr4sRgZmZVnBjMzKyKE4OZmVVxYjAzsypO\nDGZmVuX/A0zLiiWCFtR+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb9f1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ending...\n",
      "This whole code host 31 seconds...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# import the related packages\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import ndimage\n",
    "from six.moves import xrange\n",
    "from scipy.misc import imsave\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "from function import *\n",
    "#from noMnist_dataset import *\n",
    "\n",
    "\n",
    "# define the changeable parameters\n",
    "flags = tf.app.flags\n",
    "#flags.DEFINE_boolean('fake__data', False, 'the fake data used for unit testing')\n",
    "flags.DEFINE_integer('max__steps', 100, 'the number of epoch')\n",
    "flags.DEFINE_integer('dropout', 0.8, 'the value of the droup out')\n",
    "flags.DEFINE_integer('learning__rate', 0.001, 'the learning rate of the model')\n",
    "flags.DEFINE_string('optimizer', 'adam', 'the optimizer of the model')\n",
    "flags.DEFINE_string('data__dir', '/input_data', 'the direction of the data')\n",
    "flags.DEFINE_string('log__dir', 'D:/Data Minning/train_code/train/noMnist/model/', 'the direction the log file')\n",
    "FLAGS = flags.FLAGS\n",
    " \n",
    "def choose_optimizer(name):\n",
    "    if name == 'sgd':\n",
    "        optimizer = tf.train.GradientDescentOptimizer(FLAGS.learning__rate)\n",
    "    elif name == 'adam':\n",
    "        optimizer = tf.train.AdamOptimizer(FLAGS.learning__rate)\n",
    "    elif name == 'adag':\n",
    "        optimizer = tf.train.AdagradOptimizer(FLAGS.learning__rate)\n",
    "    elif name == 'adad':\n",
    "        optimizer = tf.train.AdadeltaOptimizer(FLAGS.learning__rate)\n",
    "    elif name == 'rmsp':\n",
    "        optimizer = tf.train.RMSPropOptimizer(FLAGS.learning__rate)\n",
    "    else:\n",
    "        print('please add you optimizer...')\n",
    "        raise Exception('Error...')\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def train():\n",
    "    \n",
    "    start_time = time.time()\n",
    "    num_classes = 10          # The class of the directions\n",
    "    np.random.seed(133)         \n",
    "\n",
    "    train_size = 200000       # The size of training datasets\n",
    "    valid_size = 10000        # The size of validation datasets\n",
    "    test_size = 10000         # The size of testing datasets\n",
    "    \n",
    "    image_size = 28           # Pixel width and height.\n",
    "    pixel_depth = 255.0       # Number of levels per pixel.\n",
    "    num_labels = 10           # The number of lables\n",
    "    \n",
    "    url = 'http://commondatastorage.googleapis.com/books1000/'\n",
    "    last_percent_reported = None\n",
    "    data_root = '.'           # Change me to store data elsewhere\n",
    "    \n",
    "    # Download the notMnist datasets(tar file)\n",
    "    train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "    test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)\n",
    "    \n",
    "    # Tar the file to the folders\n",
    "    train_folders = maybe_extract(train_filename)\n",
    "    test_folders = maybe_extract(test_filename)\n",
    "    \n",
    "    # Change the file to the pickle file\n",
    "    train_datasets = maybe_pickle(train_folders, 45000)\n",
    "    test_datasets = maybe_pickle(test_folders, 1800)     \n",
    "           \n",
    "    valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(train_datasets, train_size, valid_size)\n",
    "    _, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "    \n",
    "    \n",
    "    # Shuffer the datasets\n",
    "    train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "    test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "    valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)\n",
    "    \n",
    "    # Save the pickle file and check it\n",
    "    save_pickle(train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels)\n",
    "    \n",
    "    # Change the format of the datasets\n",
    "    train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "    valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "    test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "    print('Training:', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Testing:', test_dataset.shape, test_labels.shape)\n",
    "    \n",
    "    # Create a Session layer\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Input placeholders\n",
    "    with tf.name_scope('input'):\n",
    "        x = tf.placeholder(tf.float32, [None, 784], name='x_input')\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10], name='y_input')\n",
    "\n",
    "    with tf.name_scope('input_reshape'):\n",
    "        image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        tf.summary.image('image', image, 10)\n",
    "        \n",
    "    hidden1 = nn_layer(x, 784, 500, 'layer1')\n",
    "    hidden2 = nn_layer(hidden1, 500, 225, 'layer2')\n",
    "\n",
    "    with tf.name_scope('dropout'):\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        tf.summary.scalar('druoput__keep_probability', keep_prob)\n",
    "        dropped = tf.nn.dropout(hidden2, keep_prob)\n",
    "\n",
    "    # Do not apply softmax activation yet, see below.\n",
    "    y = nn_layer(dropped, 225, 10, 'layer3', act=tf.identity)\n",
    "\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "        with tf.name_scope('total'):\n",
    "            cross_entropy = tf.reduce_mean(diff)\n",
    "        tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = choose_optimizer(name = FLAGS.optimizer).minimize(cross_entropy)\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    # Merge all the summaries and write them out to the log_dir\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(FLAGS.log__dir + '/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(FLAGS.log__dir + '/test')\n",
    "    tf.global_variables_initializer().run()\n",
    "             \n",
    "    # Train the model, and also write summaries.\n",
    "    # Every 10th step, measure test-set accuracy, and write test summaries\n",
    "    # All other steps, run train_step on training data, & add training summaries\n",
    "\n",
    "    def feed_dict(train):\n",
    "        \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "        if train:\n",
    "            xs, ys = get_batch_data(data = train_dataset, label = train_labels, batch_size = 100)\n",
    "            k = FLAGS.dropout\n",
    "        else:\n",
    "            xs, ys = get_batch_data(data = test_dataset, label = test_labels, batch_size = 100)\n",
    "            k = 1.0\n",
    "        return {x: xs, y_: ys, keep_prob: k}\n",
    "    \n",
    "    accuracies = []\n",
    "    epoch = []\n",
    "    for i in xrange(FLAGS.max__steps):\n",
    "        if i % 10 == 0:  \n",
    "            # Record summaries and test-set accuracy\n",
    "            summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "            test_writer.add_summary(summary, i)\n",
    "            print('Accuracy at step %s: %s' % (i, acc))\n",
    "            \n",
    "            # Collect the accuracy and the number of epoch\n",
    "            accuracies.append(acc)\n",
    "            epoch.append(i)\n",
    "            \n",
    "        else:  \n",
    "            # Record train set summaries, and train\n",
    "            if i % 100 == 0:  \n",
    "                # Record execution stats\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run([merged, train_step],\n",
    "                                      feed_dict=feed_dict(True),\n",
    "                                      options=run_options,\n",
    "                                      run_metadata=run_metadata)\n",
    "                train_writer.add_run_metadata(run_metadata, 'step%03d' % i)\n",
    "                train_writer.add_summary(summary, i)\n",
    "                print('Adding run metadata for', i)\n",
    "            else:  \n",
    "                # Record a summary\n",
    "                summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "                train_writer.add_summary(summary, i)\n",
    "    \n",
    "    train_writer.close()\n",
    "    test_writer.close()\n",
    "    \n",
    "    # Save the checkpoint file\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, FLAGS.log__dir)\n",
    "    \n",
    "    # Plot the accuracy of the model\n",
    "    plt.plot(epoch, accuracies)\n",
    "    plt.xlabel('the number of each epoch')\n",
    "    plt.ylabel('the accuracy of each epoch')\n",
    "    plt.title('the accuracy of the model')\n",
    "    plt.show()\n",
    "    \n",
    "    print('ending...')\n",
    "    print('This whole code host %d seconds...' %(time.time() - start_time))\n",
    "\n",
    "def main(_):\n",
    "    if tf.gfile.Exists(FLAGS.log__dir):\n",
    "        tf.gfile.DeleteRecursively(FLAGS.log__dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.log__dir)\n",
    "    train()\n",
    "\n",
    "          \n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function append in module numpy.lib.function_base:\n",
      "\n",
      "append(arr, values, axis=None)\n",
      "    Append values to the end of an array.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    arr : array_like\n",
      "        Values are appended to a copy of this array.\n",
      "    values : array_like\n",
      "        These values are appended to a copy of `arr`.  It must be of the\n",
      "        correct shape (the same shape as `arr`, excluding `axis`).  If\n",
      "        `axis` is not specified, `values` can be any shape and will be\n",
      "        flattened before use.\n",
      "    axis : int, optional\n",
      "        The axis along which `values` are appended.  If `axis` is not\n",
      "        given, both `arr` and `values` are flattened before use.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    append : ndarray\n",
      "        A copy of `arr` with `values` appended to `axis`.  Note that\n",
      "        `append` does not occur in-place: a new array is allocated and\n",
      "        filled.  If `axis` is None, `out` is a flattened array.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    insert : Insert elements into an array.\n",
      "    delete : Delete elements from an array.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> np.append([1, 2, 3], [[4, 5, 6], [7, 8, 9]])\n",
      "    array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "    \n",
      "    When `axis` is specified, `values` must have the correct shape.\n",
      "    \n",
      "    >>> np.append([[1, 2, 3], [4, 5, 6]], [[7, 8, 9]], axis=0)\n",
      "    array([[1, 2, 3],\n",
      "           [4, 5, 6],\n",
      "           [7, 8, 9]])\n",
      "    >>> np.append([[1, 2, 3], [4, 5, 6]], [7, 8, 9], axis=0)\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    ValueError: arrays must have same number of dimensions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import ndimage\n",
    "from six.moves import xrange\n",
    "from six.moves import cPickle as pickle\n",
    "from IPython.display import display, Image\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "\"\"\"\n",
    "num_classes = 10          # The class of the directions\n",
    "np.random.seed(133)         \n",
    "\n",
    "train_size = 200000       # The size of training datasets\n",
    "valid_size = 10000        # The size of validation datasets\n",
    "test_size = 10000         # The size of testing datasets\n",
    "    \n",
    "image_size = 28           # Pixel width and height.\n",
    "pixel_depth = 255.0       # Number of levels per pixel.\n",
    "num_labels = 10           # The number of lables\n",
    "\n",
    "url = 'http://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = '.'           # Change me to store data elsewhere\n",
    "\n",
    "#\n",
    "\"\"\"\n",
    "global train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels\n",
    "\n",
    "class notMnist(object):\n",
    "    def __init__(self, num_classes, train_size, valid_size, test_size, image_size, \n",
    "                 pixel_depth, num_labels, url, last_percent_reported, data_root):\n",
    "        self.num_classes = num_classes\n",
    "        self.train_size = train_size\n",
    "        self.valid_size = valid_size\n",
    "        self.test_size = test_size\n",
    "        self.image_size = image_size\n",
    "        self.pixel_depth = pixel_depth\n",
    "        self.num_labels = num_labels\n",
    "        self.url = url\n",
    "        self.last_percent_reported = last_percent_reported\n",
    "        self.data_root = data_root\n",
    "        \n",
    "        # define some funtion to sample the code\n",
    "    def download_progress_hook(self, count, blockSize, totalSize):\n",
    "        \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "        slow internet connections. Reports every 5% change in download progress.\n",
    "        \"\"\"\n",
    "        #global last_percent_reported\n",
    "        percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "        if self.last_percent_reported != percent:\n",
    "            if percent % 5 == 0:\n",
    "                sys.stdout.write(\"%s%%\" % percent)\n",
    "                sys.stdout.flush()\n",
    "            else:\n",
    "                sys.stdout.write(\".\")\n",
    "                sys.stdout.flush()\n",
    "      \n",
    "        self.last_percent_reported = percent\n",
    "    \n",
    "    def maybe_download(self, filename, expected_bytes, force=False):\n",
    "        \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "        dest_filename = os.path.join(self.data_root, filename)\n",
    "        if force or not os.path.exists(dest_filename):\n",
    "            print('Attempting to download:', filename) \n",
    "            filename, _ = urlretrieve(self.url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "            print('\\nDownload Complete!')\n",
    "        statinfo = os.stat(dest_filename)\n",
    "        if statinfo.st_size == expected_bytes:\n",
    "            print('Found and verified', dest_filename)\n",
    "        else:\n",
    "            raise Exception( 'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "        return dest_filename\n",
    "\n",
    "    def maybe_extract(self, filename, force=False):\n",
    "        root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "        if os.path.isdir(root) and not force:\n",
    "        # You may override by setting force=True.\n",
    "            print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "        else:\n",
    "            print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "            tar = tarfile.open(filename)\n",
    "            sys.stdout.flush()\n",
    "            tar.extractall(self.data_root)\n",
    "            tar.close()\n",
    "        data_folders = [os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "        if os.path.isdir(os.path.join(root, d))]\n",
    "        if len(data_folders) != num_classes:\n",
    "            raise Exception( 'Expected %d folders, one per class. Found %d instead.' \n",
    "                            % (num_classes, len(data_folders)))\n",
    "        print(data_folders)\n",
    "        return data_folders\n",
    "\n",
    "\n",
    "    def make_arrays(self, nb_rows, img_size):\n",
    "        \"\"\"Change the data and lables to the arrays \"\"\"\n",
    "        if nb_rows:\n",
    "            dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "            label = np.ndarray(nb_rows, dtype=np.int32)\n",
    "        else:\n",
    "            dataset, label = None, None\n",
    "        return dataset, label\n",
    "    \n",
    "    def merge_datasets(self, pickle_files, train_size, valid_size=0):\n",
    "        num_classes = len(pickle_files)\n",
    "        valid_dataset, valid_labels = make_arrays(self.valid_size, self.image_size)\n",
    "        train_dataset, train_labels = make_arrays(self.train_size, self.image_size)\n",
    "        vsize_per_class = self.valid_size // self.num_classes\n",
    "        tsize_per_class = self.train_size // self.num_classes\n",
    "    \n",
    "        start_v, start_t = 0, 0\n",
    "        end_v, end_t = vsize_per_class, tsize_per_class\n",
    "        end_l = vsize_per_class+tsize_per_class\n",
    "        for label, pickle_file in enumerate(pickle_files):       \n",
    "            try:\n",
    "                with open(pickle_file, 'rb') as f:\n",
    "                    letter_set = pickle.load(f)\n",
    "                    np.random.shuffle(letter_set)\n",
    "                    if valid_dataset is not None:\n",
    "                        valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "                        valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "                        valid_labels[start_v:end_v] = label\n",
    "                        start_v += vsize_per_class\n",
    "                        end_v += vsize_per_class\n",
    "                    \n",
    "                    train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "                    train_dataset[start_t:end_t, :, :] = train_letter\n",
    "                    train_labels[start_t:end_t] = label\n",
    "                    start_t += tsize_per_class\n",
    "                    end_t += tsize_per_class\n",
    "            except Exception as e:\n",
    "                print('Unable to process data from', pickle_file, ':', e)\n",
    "                raise\n",
    "    \n",
    "        return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "\n",
    "    def load_letter(self, folder, min_num_images):\n",
    "        \"\"\"Load the data for a single letter label.\"\"\"\n",
    "        image_files = os.listdir(folder)\n",
    "        dataset = np.ndarray(shape=(len(image_files), self.image_size, self.image_size),dtype=np.float32)\n",
    "        print(folder)\n",
    "        num_images = 0\n",
    "        for image in image_files:\n",
    "            image_file = os.path.join(folder, image)\n",
    "            try:\n",
    "                image_data = (ndimage.imread(image_file).astype(float) - pixel_depth / 2) / pixel_depth\n",
    "                if image_data.shape != (self.image_size, self.image_size):\n",
    "                    raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "                dataset[num_images, :, :] = image_data\n",
    "                num_images = num_images + 1\n",
    "            except IOError as e:\n",
    "                print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "        \n",
    "        dataset = dataset[0:num_images, :, :]\n",
    "        if num_images < min_num_images:\n",
    "            raise Exception('Many fewer images than expected: %d < %d' % (num_images, min_num_images))\n",
    "    \n",
    "        print('Full dataset tensor:', dataset.shape)\n",
    "        print('Mean:', np.mean(dataset))\n",
    "        print('Standard deviation:', np.std(dataset))\n",
    "        return dataset\n",
    "\n",
    "    def maybe_pickle(self, data_folders, min_num_images_per_class, force=False):\n",
    "        \"\"\"Check the pickle file and load the pictures\"\"\"\n",
    "        dataset_names = []\n",
    "        for folder in data_folders:\n",
    "            set_filename = folder + '.pickle'\n",
    "            dataset_names.append(set_filename)\n",
    "            if os.path.exists(set_filename) and not force:\n",
    "                # You may override by setting force=True.\n",
    "                print('%s already present - Skipping pickling.' % set_filename)\n",
    "            else:\n",
    "                print('Pickling %s.' % set_filename)\n",
    "                dataset = load_letter(folder, min_num_images_per_class)\n",
    "                try:\n",
    "                    with open(set_filename, 'wb') as f:\n",
    "                        pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "                except Exception as e:\n",
    "                    print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "        return dataset_names\n",
    "\n",
    "    def save_pickle(self, train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels):\n",
    "        pickle_file = os.path.join(self.data_root, 'notMNIST.pickle')\n",
    "\n",
    "        try:\n",
    "            f = open(pickle_file, 'wb')\n",
    "            save = {\n",
    "            'train_dataset': train_dataset,\n",
    "            'train_labels': train_labels,\n",
    "            'valid_dataset': valid_dataset,\n",
    "            'valid_labels': valid_labels,\n",
    "            'test_dataset': test_dataset,\n",
    "            'test_labels': test_labels,\n",
    "            }\n",
    "            pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "            f.close()\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', pickle_file, ':', e)\n",
    "            raise\n",
    "    \n",
    "        statinfo = os.stat(pickle_file)\n",
    "        print('Compressed pickle size:', statinfo.st_size)\n",
    "\n",
    "    def randomize(self, dataset, labels):\n",
    "        \"\"\"Random the datas and the lables\"\"\"\n",
    "        permutation = np.random.permutation(labels.shape[0])\n",
    "        shuffled_dataset = dataset[permutation,:,:]\n",
    "        shuffled_labels = labels[permutation]\n",
    "        return shuffled_dataset, shuffled_labels\n",
    "    \n",
    "    def reformat(self, dataset, lables):\n",
    "        \"\"\"Change the shape of the datasets and lables\"\"\"\n",
    "        dataset = dataset.reshape((-1, self.image_size * self.image_size)).astype(np.float32)\n",
    "        lables = (np.arange(self.num_labels) == lables[:,None]).astype(np.float32)\n",
    "        return dataset, lables\n",
    "    \n",
    "    # We can't initialize these variables to 0 - the network will get stuck.\n",
    "    def weight_variable(self, shape):\n",
    "        \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(self, shape):\n",
    "        \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def variable_summaries(self, var):\n",
    "        \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "    \n",
    "    def nn_layer(self, input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "        \"\"\"Reusable code for making a simple neural net layer.\n",
    "        It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
    "        It also sets up name scoping so that the resultant graph is easy to read,\n",
    "        and adds a number of summary ops.\n",
    "        \"\"\"\n",
    "        # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "        with tf.name_scope(layer_name):\n",
    "            # This Variable will hold the state of the weights for the layer\n",
    "            with tf.name_scope('weights'):\n",
    "                weights = weight_variable([input_dim, output_dim])\n",
    "                variable_summaries(weights)\n",
    "            with tf.name_scope('biases'):\n",
    "                biases = bias_variable([output_dim])\n",
    "                variable_summaries(biases)\n",
    "            with tf.name_scope('Wx_plus_b'):\n",
    "                preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "            tf.summary.histogram('pre_activations', preactivate)\n",
    "            activations = act(preactivate, name='activation')\n",
    "            tf.summary.histogram('activations', activations)\n",
    "        return activations\n",
    " \n",
    "    def get_batch_data(self, data,label,batch_size):\n",
    "        \"\"\"Get the batch datas and the lables\"\"\"\n",
    "        start_index = np.random.randint(0, len(data) - batch_size)\n",
    "        return data[start_index : start_index + batch_size], label[start_index : start_index + batch_size]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified .\\notMNIST_large.tar.gz\n",
      "Found and verified .\\notMNIST_small.tar.gz\n",
      ".\\notMNIST_large already present - Skipping extraction of .\\notMNIST_large.tar.gz.\n",
      "['.\\\\notMNIST_large\\\\A', '.\\\\notMNIST_large\\\\B', '.\\\\notMNIST_large\\\\C', '.\\\\notMNIST_large\\\\D', '.\\\\notMNIST_large\\\\E', '.\\\\notMNIST_large\\\\F', '.\\\\notMNIST_large\\\\G', '.\\\\notMNIST_large\\\\H', '.\\\\notMNIST_large\\\\I', '.\\\\notMNIST_large\\\\J']\n",
      ".\\notMNIST_small already present - Skipping extraction of .\\notMNIST_small.tar.gz.\n",
      "['.\\\\notMNIST_small\\\\A', '.\\\\notMNIST_small\\\\B', '.\\\\notMNIST_small\\\\C', '.\\\\notMNIST_small\\\\D', '.\\\\notMNIST_small\\\\E', '.\\\\notMNIST_small\\\\F', '.\\\\notMNIST_small\\\\G', '.\\\\notMNIST_small\\\\H', '.\\\\notMNIST_small\\\\I', '.\\\\notMNIST_small\\\\J']\n",
      ".\\notMNIST_large\\A.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\B.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\C.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\D.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\E.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\F.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\G.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\H.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\I.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\J.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\A.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\B.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\C.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\D.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\E.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\F.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\G.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\H.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\I.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\J.pickle already present - Skipping pickling.\n",
      "Compressed pickle size: 690800503\n",
      "Training: (200000, 784) (200000, 10)\n",
      "Validation: (10000, 784) (10000, 10)\n",
      "Testing: (10000, 784) (10000, 10)\n",
      "Accuracy at step 0: 0.09\n",
      "Accuracy at step 10: 0.68\n",
      "Accuracy at step 20: 0.82\n",
      "Accuracy at step 30: 0.86\n",
      "Accuracy at step 40: 0.89\n",
      "Accuracy at step 50: 0.89\n",
      "Accuracy at step 60: 0.91\n",
      "Accuracy at step 70: 0.87\n",
      "Accuracy at step 80: 0.88\n",
      "Accuracy at step 90: 0.88\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEZCAYAAACTsIJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd4fc240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ending...\n",
      "This whole code host 33 seconds...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# import the related packages\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import ndimage\n",
    "from six.moves import xrange\n",
    "from scipy.misc import imsave\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "from notMnist_class import *\n",
    "\n",
    "# define the changeable parameters\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_integer('max__steps', 100, 'the number of epoch')\n",
    "flags.DEFINE_integer('dropout', 0.8, 'the value of the droup out')\n",
    "flags.DEFINE_integer('learning__rate', 0.001, 'the learning rate of the model')\n",
    "flags.DEFINE_string('optimizer', 'adam', 'the optimizer of the model')\n",
    "flags.DEFINE_string('data__dir', '/input_data', 'the direction of the data')\n",
    "flags.DEFINE_string('log__dir', 'D:/Data Minning/train_code/train/noMnist/model/', 'the direction the log file')\n",
    "FLAGS = flags.FLAGS\n",
    " \n",
    "def choose_optimizer(name):\n",
    "    if name == 'sgd':\n",
    "        optimizer = tf.train.GradientDescentOptimizer(FLAGS.learning__rate)\n",
    "    elif name == 'adam':\n",
    "        optimizer = tf.train.AdamOptimizer(FLAGS.learning__rate)\n",
    "    elif name == 'adag':\n",
    "        optimizer = tf.train.AdagradOptimizer(FLAGS.learning__rate)\n",
    "    elif name == 'adad':\n",
    "        optimizer = tf.train.AdadeltaOptimizer(FLAGS.learning__rate)\n",
    "    elif name == 'rmsp':\n",
    "        optimizer = tf.train.RMSPropOptimizer(FLAGS.learning__rate)\n",
    "    else:\n",
    "        print('please add you optimizer...')\n",
    "        raise Exception('Error...')\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def train():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    np.random.seed(133)\n",
    "    num_classes = 10          # The class of the directions      \n",
    "\n",
    "    train_size = 200000       # The size of training datasets\n",
    "    valid_size = 10000        # The size of validation datasets\n",
    "    test_size = 10000         # The size of testing datasets\n",
    "    \n",
    "    image_size = 28           # Pixel width and height.\n",
    "    pixel_depth = 255.0       # Number of levels per pixel.\n",
    "    num_labels = 10           # The number of lables\n",
    "    \n",
    "    url = 'http://commondatastorage.googleapis.com/books1000/'\n",
    "    last_percent_reported = None\n",
    "    data_root = '.'           # Change me to store data elsewhere\n",
    "    \n",
    "    # Instance a object \n",
    "    notMnist_object  = notMnist(num_classes = 10, train_size = 200000, valid_size = 10000, test_size = 10000,\n",
    "                                image_size = 28, pixel_depth = 255.0, num_labels = 10, last_percent_reported = None,\n",
    "                                url = 'http://commondatastorage.googleapis.com/books1000/', data_root = '.')\n",
    "    \n",
    "    # Download the notMnist datasets(tar file)\n",
    "    train_filename = notMnist_object.maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "    test_filename = notMnist_object.maybe_download('notMNIST_small.tar.gz', 8458043)\n",
    "    \n",
    "    # Tar the file to the folders\n",
    "    train_folders = notMnist_object.maybe_extract(train_filename)\n",
    "    test_folders = notMnist_object.maybe_extract(test_filename)\n",
    "    \n",
    "    # Change the file to the pickle file\n",
    "    train_datasets = notMnist_object.maybe_pickle(train_folders, 45000)\n",
    "    test_datasets = notMnist_object.maybe_pickle(test_folders, 1800)     \n",
    "           \n",
    "    valid_dataset, valid_labels, train_dataset, train_labels = notMnist_object.merge_datasets(train_datasets, train_size, valid_size)\n",
    "    _, _, test_dataset, test_labels = notMnist_object.merge_datasets(test_datasets, test_size)\n",
    "    \n",
    "    \n",
    "    # Shuffer the datasets\n",
    "    train_dataset, train_labels = notMnist_object.randomize(train_dataset, train_labels)\n",
    "    test_dataset, test_labels = notMnist_object.randomize(test_dataset, test_labels)\n",
    "    valid_dataset, valid_labels = notMnist_object.randomize(valid_dataset, valid_labels)\n",
    "    \n",
    "    # Save the pickle file and check it\n",
    "    notMnist_object.save_pickle(train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels)\n",
    "    \n",
    "    # Change the format of the datasets\n",
    "    train_dataset, train_labels = notMnist_object.reformat(train_dataset, train_labels)\n",
    "    valid_dataset, valid_labels = notMnist_object.reformat(valid_dataset, valid_labels)\n",
    "    test_dataset, test_labels = notMnist_object.reformat(test_dataset, test_labels)\n",
    "    print('Training:', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Testing:', test_dataset.shape, test_labels.shape)\n",
    "    \n",
    "    # Create a Session layer\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Input placeholders\n",
    "    with tf.name_scope('input'):\n",
    "        x = tf.placeholder(tf.float32, [None, 784], name='x_input')\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10], name='y_input')\n",
    "\n",
    "    with tf.name_scope('input_reshape'):\n",
    "        image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        tf.summary.image('image', image, 10)\n",
    "        \n",
    "    hidden1 = notMnist_object.nn_layer(x, 784, 500, 'layer1')\n",
    "    hidden2 = notMnist_object.nn_layer(hidden1, 500, 225, 'layer2')\n",
    "\n",
    "    with tf.name_scope('dropout'):\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        tf.summary.scalar('druoput__keep_probability', keep_prob)\n",
    "        dropped = tf.nn.dropout(hidden2, keep_prob)\n",
    "\n",
    "    # Do not apply softmax activation yet, see below.\n",
    "    y = notMnist_object.nn_layer(dropped, 225, 10, 'layer3', act=tf.identity)\n",
    "\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "        with tf.name_scope('total'):\n",
    "            cross_entropy = tf.reduce_mean(diff)\n",
    "        tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = choose_optimizer(name = FLAGS.optimizer).minimize(cross_entropy)\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    # Merge all the summaries and write them out to the log_dir\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(FLAGS.log__dir + '/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(FLAGS.log__dir + '/test')\n",
    "    tf.global_variables_initializer().run()\n",
    "             \n",
    "    # Train the model, and also write summaries.\n",
    "    # Every 10th step, measure test-set accuracy, and write test summaries\n",
    "    # All other steps, run train_step on training data, & add training summaries\n",
    "\n",
    "    def feed_dict(train):\n",
    "        \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "        if train:\n",
    "            xs, ys = notMnist_object.get_batch_data(data = train_dataset, label = train_labels, batch_size = 100)\n",
    "            k = FLAGS.dropout\n",
    "        else:\n",
    "            xs, ys = notMnist_object.get_batch_data(data = test_dataset, label = test_labels, batch_size = 100)\n",
    "            k = 1.0\n",
    "        return {x: xs, y_: ys, keep_prob: k}\n",
    "    \n",
    "    accuracies = []\n",
    "    epoch = []\n",
    "    for i in xrange(FLAGS.max__steps):\n",
    "        if i % 10 == 0:  \n",
    "            # Record summaries and test-set accuracy\n",
    "            summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "            test_writer.add_summary(summary, i)\n",
    "            print('Accuracy at step %s: %s' % (i, acc))\n",
    "            \n",
    "            # Collect the accuracy and the number of epoch\n",
    "            accuracies.append(acc)\n",
    "            epoch.append(i)\n",
    "            \n",
    "        else:  \n",
    "            # Record train set summaries, and train\n",
    "            if i % 100 == 0:  \n",
    "                # Record execution stats\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run([merged, train_step],\n",
    "                                      feed_dict=feed_dict(True),\n",
    "                                      options=run_options,\n",
    "                                      run_metadata=run_metadata)\n",
    "                train_writer.add_run_metadata(run_metadata, 'step%03d' % i)\n",
    "                train_writer.add_summary(summary, i)\n",
    "                print('Adding run metadata for', i)\n",
    "            else:  \n",
    "                # Record a summary\n",
    "                summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "                train_writer.add_summary(summary, i)\n",
    "    \n",
    "    train_writer.close()\n",
    "    test_writer.close()\n",
    "    \n",
    "    # Save the checkpoint file\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, FLAGS.log__dir)\n",
    "    \n",
    "    # Plot the accuracy of the model\n",
    "    plt.plot(epoch, accuracies)\n",
    "    plt.xlabel('the number of each epoch')\n",
    "    plt.ylabel('the accuracy of each epoch')\n",
    "    plt.title('the accuracy of the model')\n",
    "    plt.show()\n",
    "    \n",
    "    print('ending...')\n",
    "    print('The whole compute host %d seconds...' %(time.time() - start_time))\n",
    "\n",
    "def main(_):\n",
    "    if tf.gfile.Exists(FLAGS.log__dir):\n",
    "        tf.gfile.DeleteRecursively(FLAGS.log__dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.log__dir)\n",
    "    train()\n",
    "\n",
    "          \n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified .\\notMNIST_large.tar.gz\n",
      "Found and verified .\\notMNIST_small.tar.gz\n",
      ".\\notMNIST_large already present - Skipping extraction of .\\notMNIST_large.tar.gz.\n",
      "['.\\\\notMNIST_large\\\\A', '.\\\\notMNIST_large\\\\B', '.\\\\notMNIST_large\\\\C', '.\\\\notMNIST_large\\\\D', '.\\\\notMNIST_large\\\\E', '.\\\\notMNIST_large\\\\F', '.\\\\notMNIST_large\\\\G', '.\\\\notMNIST_large\\\\H', '.\\\\notMNIST_large\\\\I', '.\\\\notMNIST_large\\\\J']\n",
      ".\\notMNIST_small already present - Skipping extraction of .\\notMNIST_small.tar.gz.\n",
      "['.\\\\notMNIST_small\\\\A', '.\\\\notMNIST_small\\\\B', '.\\\\notMNIST_small\\\\C', '.\\\\notMNIST_small\\\\D', '.\\\\notMNIST_small\\\\E', '.\\\\notMNIST_small\\\\F', '.\\\\notMNIST_small\\\\G', '.\\\\notMNIST_small\\\\H', '.\\\\notMNIST_small\\\\I', '.\\\\notMNIST_small\\\\J']\n",
      ".\\notMNIST_large\\A.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\B.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\C.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\D.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\E.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\F.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\G.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\H.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\I.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\J.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\A.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\B.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\C.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\D.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\E.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\F.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\G.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\H.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\I.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_small\\J.pickle already present - Skipping pickling.\n",
      "Compressed pickle size: 690800503\n",
      "Training: (200000, 784) (200000, 10)\n",
      "Validation: (10000, 784) (10000, 10)\n",
      "Testing: (10000, 784) (10000, 10)\n",
      "Accuracy at step 0: 0.01\n",
      "Accuracy at step 10: 0.16\n",
      "Accuracy at step 20: 0.25\n",
      "Accuracy at step 30: 0.35\n",
      "Accuracy at step 40: 0.49\n",
      "Accuracy at step 50: 0.5\n",
      "Accuracy at step 60: 0.69\n",
      "Accuracy at step 70: 0.78\n",
      "Accuracy at step 80: 0.81\n",
      "Accuracy at step 90: 0.78\n",
      "Accuracy at step 100: 0.81\n",
      "Accuracy at step 110: 0.81\n",
      "Accuracy at step 120: 0.86\n",
      "Accuracy at step 130: 0.8\n",
      "Accuracy at step 140: 0.85\n",
      "Accuracy at step 150: 0.81\n",
      "Accuracy at step 160: 0.85\n",
      "Accuracy at step 170: 0.86\n",
      "Accuracy at step 180: 0.89\n",
      "Accuracy at step 190: 0.89\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEZCAYAAABiu9n+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xdf5fe80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ending...\n",
      "The whole compute host 65 seconds...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# import the related packages\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import ndimage\n",
    "from six.moves import xrange\n",
    "from scipy.misc import imsave\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "from notMnist_class_cnn import *\n",
    "\n",
    "# define the changeable parameters\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_integer('max__steps', 200, 'the number of epoch')\n",
    "flags.DEFINE_integer('dropout', 0.8, 'the value of the droup out')\n",
    "flags.DEFINE_integer('learning__rate', 0.001, 'the learning rate of the model')\n",
    "flags.DEFINE_string('optimizer', 'adam', 'the optimizer of the model')\n",
    "flags.DEFINE_string('data__dir', '/input_data', 'the direction of the data')\n",
    "flags.DEFINE_string('log__dir', 'D:/Data Minning/train_code/train/noMnist/model/', 'the direction the log file')\n",
    "FLAGS = flags.FLAGS\n",
    " \n",
    "def choose_optimizer(name):\n",
    "    if name == 'sgd':\n",
    "        optimizer = tf.train.GradientDescentOptimizer(FLAGS.learning__rate)\n",
    "    elif name == 'adam':\n",
    "        optimizer = tf.train.AdamOptimizer(FLAGS.learning__rate)\n",
    "    elif name == 'adag':\n",
    "        optimizer = tf.train.AdagradOptimizer(FLAGS.learning__rate)\n",
    "    elif name == 'adad':\n",
    "        optimizer = tf.train.AdadeltaOptimizer(FLAGS.learning__rate)\n",
    "    elif name == 'rmsp':\n",
    "        optimizer = tf.train.RMSPropOptimizer(FLAGS.learning__rate)\n",
    "    else:\n",
    "        print('please add you optimizer...')\n",
    "        raise Exception('Error...')\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def train():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    np.random.seed(133)\n",
    "    num_classes = 10          # The class of the directions      \n",
    "\n",
    "    train_size = 200000       # The size of training datasets\n",
    "    valid_size = 10000        # The size of validation datasets\n",
    "    test_size = 10000         # The size of testing datasets\n",
    "    \n",
    "    image_size = 28           # Pixel width and height.\n",
    "    pixel_depth = 255.0       # Number of levels per pixel.\n",
    "    num_labels = 10           # The number of lables\n",
    "    \n",
    "    depth = 16                # The depth of each layer\n",
    "    batch_size = 16           # The batch size of each layer\n",
    "    patch_size = 5            # The filter ize of each layer\n",
    "    num_hidden = 64           # The size of hidden layer\n",
    "    num_channels = 1          # The channel of image\n",
    "    \n",
    "    url = 'http://commondatastorage.googleapis.com/books1000/'\n",
    "    last_percent_reported = None\n",
    "    data_root = '.'           # Change me to store data elsewhere\n",
    "    \n",
    "    # Instance a object \n",
    "    notMnist_object  = notMnist(num_classes = 10, train_size = 200000, valid_size = 10000, test_size = 10000,\n",
    "                                image_size = 28, pixel_depth = 255.0, num_labels = 10, last_percent_reported = None,\n",
    "                                url = 'http://commondatastorage.googleapis.com/books1000/', data_root = '.')\n",
    "    \n",
    "    # Download the notMnist datasets(tar file)\n",
    "    train_filename = notMnist_object.maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "    test_filename = notMnist_object.maybe_download('notMNIST_small.tar.gz', 8458043)\n",
    "    \n",
    "    # Tar the file to the folders\n",
    "    train_folders = notMnist_object.maybe_extract(train_filename)\n",
    "    test_folders = notMnist_object.maybe_extract(test_filename)\n",
    "    \n",
    "    # Change the file to the pickle file\n",
    "    train_datasets = notMnist_object.maybe_pickle(train_folders, 45000)\n",
    "    test_datasets = notMnist_object.maybe_pickle(test_folders, 1800)     \n",
    "           \n",
    "    valid_dataset, valid_labels, train_dataset, train_labels = notMnist_object.merge_datasets(train_datasets, train_size, valid_size)\n",
    "    _, _, test_dataset, test_labels = notMnist_object.merge_datasets(test_datasets, test_size)\n",
    "    \n",
    "    \n",
    "    # Shuffer the datasets\n",
    "    train_dataset, train_labels = notMnist_object.randomize(train_dataset, train_labels)\n",
    "    test_dataset, test_labels = notMnist_object.randomize(test_dataset, test_labels)\n",
    "    valid_dataset, valid_labels = notMnist_object.randomize(valid_dataset, valid_labels)\n",
    "    \n",
    "    # Save the pickle file and check it\n",
    "    notMnist_object.save_pickle(train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels)\n",
    "    \n",
    "    # Change the format of the datasets\n",
    "    train_dataset, train_labels = notMnist_object.reformat(train_dataset, train_labels)\n",
    "    valid_dataset, valid_labels = notMnist_object.reformat(valid_dataset, valid_labels)\n",
    "    test_dataset, test_labels = notMnist_object.reformat(test_dataset, test_labels)\n",
    "    print('Training:', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Testing:', test_dataset.shape, test_labels.shape)\n",
    "    \n",
    "    # Create a Session layer\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Input placeholders\n",
    "    with tf.name_scope('input'):\n",
    "        x = tf.placeholder(tf.float32, [None, 784], name='x_input')\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10], name='y_input')\n",
    "\n",
    "    with tf.name_scope('input_reshape'):\n",
    "        image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        tf.summary.image('image', image, 10)\n",
    "    \n",
    "    weights1, weights2, weights3, weights4, biases1, biases2, biases3, biases4 = notMnist_object.layer_weights(\n",
    "                                  patch_size = patch_size, num_channels = num_channels, depth = depth, \n",
    "                                  image_size = image_size, num_hidden = num_hidden, num_labels = num_labels)\n",
    "    \n",
    "    y = notMnist_object.notMnist_CNN(data = image, weights1  = weights1 , weights2 = weights2, weights3 = weights3, \n",
    "                                     weights4 = weights4, biases1 = biases1, biases2 = biases2, biases3 = biases3, \n",
    "                                     biases4 = biases4)\n",
    "    \n",
    "    with tf.name_scope('dropout'):\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        tf.summary.scalar('druoput__keep_probability', keep_prob)\n",
    "        \n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "        with tf.name_scope('total'):\n",
    "            cross_entropy = tf.reduce_mean(diff)\n",
    "        tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = choose_optimizer(name = FLAGS.optimizer).minimize(cross_entropy)\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    # Merge all the summaries and write them out to the log_dir\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(FLAGS.log__dir + '/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(FLAGS.log__dir + '/test')\n",
    "    tf.global_variables_initializer().run()\n",
    "             \n",
    "    # Train the model, and also write summaries.\n",
    "    # Every 10th step, measure test-set accuracy, and write test summaries\n",
    "    # All other steps, run train_step on training data, & add training summaries\n",
    "\n",
    "    def feed_dict(train):\n",
    "        \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "        if train:\n",
    "            xs, ys = notMnist_object.get_batch_data(data = train_dataset, label = train_labels, batch_size = 100)\n",
    "            k = FLAGS.dropout\n",
    "        else:\n",
    "            xs, ys = notMnist_object.get_batch_data(data = test_dataset, label = test_labels, batch_size = 100)\n",
    "            k = 1.0\n",
    "        return {x: xs, y_: ys, keep_prob: k}\n",
    "    \n",
    "    accuracies = []\n",
    "    epoch = []\n",
    "    for i in xrange(FLAGS.max__steps):\n",
    "        if i % 10 == 0:  \n",
    "            # Record summaries and test-set accuracy\n",
    "            summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "            test_writer.add_summary(summary, i)\n",
    "            print('Accuracy at step %s: %s' % (i, acc))\n",
    "            \n",
    "            # Collect the accuracy and the number of epoch\n",
    "            accuracies.append(acc)\n",
    "            epoch.append(i)\n",
    "            \n",
    "        else:  \n",
    "            # Record train set summaries, and train\n",
    "            if i % 100 == 0:  \n",
    "                # Record execution stats\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run([merged, train_step],\n",
    "                                      feed_dict=feed_dict(True),\n",
    "                                      options=run_options,\n",
    "                                      run_metadata=run_metadata)\n",
    "                train_writer.add_run_metadata(run_metadata, 'step%03d' % i)\n",
    "                train_writer.add_summary(summary, i)\n",
    "                print('Adding run metadata for', i)\n",
    "            else:  \n",
    "                # Record a summary\n",
    "                summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "                train_writer.add_summary(summary, i)\n",
    "    \n",
    "    train_writer.close()\n",
    "    test_writer.close()\n",
    "    \n",
    "    # Save the checkpoint file\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, FLAGS.log__dir)\n",
    "    \n",
    "    # Plot the accuracy of the model\n",
    "    plt.plot(epoch, accuracies)\n",
    "    plt.xlabel('the number of each epoch')\n",
    "    plt.ylabel('the accuracy of each epoch')\n",
    "    plt.title('the accuracy of the model')\n",
    "    plt.show()\n",
    "    \n",
    "    print('ending...')\n",
    "    print('The whole compute host %d seconds...' %(time.time() - start_time))\n",
    "\n",
    "def main(_):\n",
    "    if tf.gfile.Exists(FLAGS.log__dir):\n",
    "        tf.gfile.DeleteRecursively(FLAGS.log__dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.log__dir)\n",
    "    train()\n",
    "\n",
    "          \n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import ndimage\n",
    "from six.moves import xrange\n",
    "from scipy.misc import imsave\n",
    "from six.moves import cPickle as pickle\n",
    "from IPython.display import display, Image\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "class notMnist(object):\n",
    "    def __init__(self, num_classes, train_size, valid_size, test_size, image_size, \n",
    "                 pixel_depth, num_labels, url, last_percent_reported, data_root):\n",
    "        self.num_classes = num_classes\n",
    "        self.train_size = train_size\n",
    "        self.valid_size = valid_size\n",
    "        self.test_size = test_size\n",
    "        self.image_size = image_size\n",
    "        self.pixel_depth = pixel_depth\n",
    "        self.num_labels = num_labels\n",
    "        self.url = url\n",
    "        self.last_percent_reported = last_percent_reported\n",
    "        self.data_root = data_root\n",
    "        \n",
    "        # define some funtion to sample the code\n",
    "    def download_progress_hook(self, count, blockSize, totalSize):\n",
    "        \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "        slow internet connections. Reports every 5% change in download progress.\n",
    "        \"\"\"\n",
    "        #global last_percent_reported\n",
    "        percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "        if self.last_percent_reported != percent:\n",
    "            if percent % 5 == 0:\n",
    "                sys.stdout.write(\"%s%%\" % percent)\n",
    "                sys.stdout.flush()\n",
    "            else:\n",
    "                sys.stdout.write(\".\")\n",
    "                sys.stdout.flush()\n",
    "      \n",
    "        self.last_percent_reported = percent\n",
    "    \n",
    "    def maybe_download(self, filename, expected_bytes, force=False):\n",
    "        \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "        dest_filename = os.path.join(self.data_root, filename)\n",
    "        if force or not os.path.exists(dest_filename):\n",
    "            print('Attempting to download:', filename) \n",
    "            filename, _ = urlretrieve(self.url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "            print('\\nDownload Complete!')\n",
    "        statinfo = os.stat(dest_filename)\n",
    "        if statinfo.st_size == expected_bytes:\n",
    "            print('Found and verified', dest_filename)\n",
    "        else:\n",
    "            raise Exception( 'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "        return dest_filename\n",
    "\n",
    "    def maybe_extract(self, filename, force=False):\n",
    "        root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "        if os.path.isdir(root) and not force:\n",
    "        # You may override by setting force=True.\n",
    "            print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "        else:\n",
    "            print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "            tar = tarfile.open(filename)\n",
    "            sys.stdout.flush()\n",
    "            tar.extractall(self.data_root)\n",
    "            tar.close()\n",
    "        data_folders = [os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "        if os.path.isdir(os.path.join(root, d))]\n",
    "        if len(data_folders) != self.num_classes:\n",
    "            raise Exception( 'Expected %d folders, one per class. Found %d instead.' \n",
    "                            % (self.num_classes, len(data_folders)))\n",
    "        print(data_folders)\n",
    "        return data_folders\n",
    "\n",
    "\n",
    "    def make_arrays(self, nb_rows, img_size):\n",
    "        \"\"\"Change the data and lables to the arrays \"\"\"\n",
    "        if nb_rows:\n",
    "            dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "            label = np.ndarray(nb_rows, dtype=np.int32)\n",
    "        else:\n",
    "            dataset, label = None, None\n",
    "        return dataset, label\n",
    "    \n",
    "    def merge_datasets(self, pickle_files, train_size, valid_size=0):\n",
    "        num_classes = len(pickle_files)\n",
    "        valid_dataset, valid_labels = self.make_arrays(valid_size, self.image_size)\n",
    "        train_dataset, train_labels = self.make_arrays(train_size, self.image_size)\n",
    "        vsize_per_class = valid_size // self.num_classes\n",
    "        tsize_per_class = train_size // self.num_classes\n",
    "    \n",
    "        start_v, start_t = 0, 0\n",
    "        end_v, end_t = vsize_per_class, tsize_per_class\n",
    "        end_l = vsize_per_class+tsize_per_class\n",
    "        for label, pickle_file in enumerate(pickle_files):       \n",
    "            try:\n",
    "                with open(pickle_file, 'rb') as f:\n",
    "                    letter_set = pickle.load(f)\n",
    "                    np.random.shuffle(letter_set)\n",
    "                    if valid_dataset is not None:\n",
    "                        valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "                        valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "                        valid_labels[start_v:end_v] = label\n",
    "                        start_v += vsize_per_class\n",
    "                        end_v += vsize_per_class\n",
    "                    \n",
    "                    train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "                    train_dataset[start_t:end_t, :, :] = train_letter\n",
    "                    train_labels[start_t:end_t] = label\n",
    "                    start_t += tsize_per_class\n",
    "                    end_t += tsize_per_class\n",
    "            except Exception as e:\n",
    "                print('Unable to process data from', pickle_file, ':', e)\n",
    "                raise\n",
    "    \n",
    "        return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "\n",
    "    def load_letter(self, folder, min_num_images):\n",
    "        \"\"\"Load the data for a single letter label.\"\"\"\n",
    "        image_files = os.listdir(folder)\n",
    "        dataset = np.ndarray(shape=(len(image_files), self.image_size, self.image_size),dtype=np.float32)\n",
    "        print(folder)\n",
    "        num_images = 0\n",
    "        for image in image_files:\n",
    "            image_file = os.path.join(folder, image)\n",
    "            try:\n",
    "                image_data = (ndimage.imread(image_file).astype(float) - pixel_depth / 2) / pixel_depth\n",
    "                if image_data.shape != (self.image_size, self.image_size):\n",
    "                    raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "                dataset[num_images, :, :] = image_data\n",
    "                num_images = num_images + 1\n",
    "            except IOError as e:\n",
    "                print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "        \n",
    "        dataset = dataset[0:num_images, :, :]\n",
    "        if num_images < min_num_images:\n",
    "            raise Exception('Many fewer images than expected: %d < %d' % (num_images, min_num_images))\n",
    "    \n",
    "        print('Full dataset tensor:', dataset.shape)\n",
    "        print('Mean:', np.mean(dataset))\n",
    "        print('Standard deviation:', np.std(dataset))\n",
    "        return dataset\n",
    "\n",
    "    def maybe_pickle(self, data_folders, min_num_images_per_class, force=False):\n",
    "        \"\"\"Check the pickle file and load the pictures\"\"\"\n",
    "        dataset_names = []\n",
    "        for folder in data_folders:\n",
    "            set_filename = folder + '.pickle'\n",
    "            dataset_names.append(set_filename)\n",
    "            if os.path.exists(set_filename) and not force:\n",
    "                # You may override by setting force=True.\n",
    "                print('%s already present - Skipping pickling.' % set_filename)\n",
    "            else:\n",
    "                print('Pickling %s.' % set_filename)\n",
    "                dataset = load_letter(folder, min_num_images_per_class)\n",
    "                try:\n",
    "                    with open(set_filename, 'wb') as f:\n",
    "                        pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "                except Exception as e:\n",
    "                    print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "        return dataset_names\n",
    "\n",
    "    def save_pickle(self, train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels):\n",
    "        pickle_file = os.path.join(self.data_root, 'notMNIST.pickle')\n",
    "\n",
    "        try:\n",
    "            f = open(pickle_file, 'wb')\n",
    "            save = {\n",
    "            'train_dataset': train_dataset,\n",
    "            'train_labels': train_labels,\n",
    "            'valid_dataset': valid_dataset,\n",
    "            'valid_labels': valid_labels,\n",
    "            'test_dataset': test_dataset,\n",
    "            'test_labels': test_labels,\n",
    "            }\n",
    "            pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "            f.close()\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', pickle_file, ':', e)\n",
    "            raise\n",
    "    \n",
    "        statinfo = os.stat(pickle_file)\n",
    "        print('Compressed pickle size:', statinfo.st_size)\n",
    "\n",
    "    def randomize(self, dataset, labels):\n",
    "        \"\"\"Random the datas and the lables\"\"\"\n",
    "        permutation = np.random.permutation(labels.shape[0])\n",
    "        shuffled_dataset = dataset[permutation,:,:]\n",
    "        shuffled_labels = labels[permutation]\n",
    "        return shuffled_dataset, shuffled_labels\n",
    "    \n",
    "    def reformat(self, dataset, lables):\n",
    "        \"\"\"Change the shape of the datasets and lables\"\"\"\n",
    "        dataset = dataset.reshape((-1, self.image_size * self.image_size)).astype(np.float32)\n",
    "        lables = (np.arange(self.num_labels) == lables[:,None]).astype(np.float32)\n",
    "        return dataset, lables\n",
    "    \n",
    "    # We can't initialize these variables to 0 - the network will get stuck.\n",
    "    def weight_variable(self, shape):\n",
    "        \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(self, shape, value):\n",
    "        \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "        initial = tf.constant(value, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def variable_summaries(self, var):\n",
    "        \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "        \n",
    "    # Define the weights and the biases of the layers \n",
    "    def layer_weights(self, patch_size, num_channels, depth, image_size, num_hidden, num_labels):\n",
    "        with tf.name_scope('layer1'):\n",
    "            weights1 = self.weight_variable([patch_size, patch_size, num_channels, depth])\n",
    "            biases1 = self.bias_variable([depth], value = 0.0)\n",
    "            self.variable_summaries(weights1)\n",
    "            self.variable_summaries(biases1)\n",
    "        \n",
    "        with tf.name_scope('layer2'):\n",
    "            weights2 = self.weight_variable([patch_size, patch_size, depth, depth])\n",
    "            biases2 = self.bias_variable([depth], value = 1.0)\n",
    "            self.variable_summaries(weights2)\n",
    "            self.variable_summaries(biases2)\n",
    "        \n",
    "        with tf.name_scope('layer3'):\n",
    "            weights3 = self.weight_variable([image_size // 4 * image_size // 4 * depth, num_hidden])\n",
    "            biases3 = self.bias_variable([num_hidden], value = 1.0)\n",
    "            self.variable_summaries(weights3)\n",
    "            self.variable_summaries(biases3)\n",
    "        \n",
    "        with tf.name_scope('layer4'):\n",
    "            weights4 = self.weight_variable([num_hidden, num_labels])\n",
    "            biases4 = self.bias_variable([num_labels], value = 1.0)\n",
    "            self.variable_summaries(weights4)\n",
    "            self.variable_summaries(biases4)\n",
    "        return weights1, weights2, weights3, weights4, biases1, biases2, biases3, biases4\n",
    "    \n",
    "    # Build the CNN model\n",
    "    def notMnist_CNN(self, data, weights1, weights2, weights3, weights4, biases1, biases2, biases3, biases4):\n",
    "        conv = tf.nn.conv2d(data, weights1, [1, 1, 1, 1], padding='SAME')\n",
    "        pool = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(pool + biases1)\n",
    "\n",
    "        conv = tf.nn.conv2d(hidden, weights2, [1, 1, 1, 1], padding='SAME')\n",
    "        pool = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(pool + biases2)\n",
    "\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [-1, shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, weights3) + biases3)\n",
    "        drop = tf.nn.dropout(hidden, 1)\n",
    "\n",
    "        output_layer = tf.matmul(drop, weights4) + biases4\n",
    "        return output_layer\n",
    "    \n",
    " \n",
    "    def get_batch_data(self, data,label,batch_size):\n",
    "        \"\"\"Get the batch datas and the lables\"\"\"\n",
    "        start_index = np.random.randint(0, len(data) - batch_size)\n",
    "        return data[start_index : start_index + batch_size], label[start_index : start_index + batch_size]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imsave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function imsave in module scipy.misc.pilutil:\n",
      "\n",
      "imsave(name, arr, format=None)\n",
      "    Save an array as an image.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    name : str or file object\n",
      "        Output file name or file object.\n",
      "    arr : ndarray, MxN or MxNx3 or MxNx4\n",
      "        Array containing image values.  If the shape is ``MxN``, the array\n",
      "        represents a grey-level image.  Shape ``MxNx3`` stores the red, green\n",
      "        and blue bands along the last dimension.  An alpha layer may be\n",
      "        included, specified as the last colour band of an ``MxNx4`` array.\n",
      "    format : str\n",
      "        Image format. If omitted, the format to use is determined from the\n",
      "        file name extension. If a file object was used instead of a file name,\n",
      "        this parameter should always be used.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Construct an array of gradient intensity values and save to file:\n",
      "    \n",
      "    >>> from scipy.misc import imsave\n",
      "    >>> x = np.zeros((255, 255))\n",
      "    >>> x = np.zeros((255, 255), dtype=np.uint8)\n",
      "    >>> x[:] = np.arange(255)\n",
      "    >>> imsave('gradient.png', x)\n",
      "    \n",
      "    Construct an array with three colour bands (R, G, B) and store to file:\n",
      "    \n",
      "    >>> rgb = np.zeros((255, 255, 3), dtype=np.uint8)\n",
      "    >>> rgb[..., 0] = np.arange(255)\n",
      "    >>> rgb[..., 1] = 55\n",
      "    >>> rgb[..., 2] = 1 - np.arange(255)\n",
      "    >>> imsave('rgb_gradient.png', rgb)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(imsave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reshape in module tensorflow.python.ops.gen_array_ops:\n",
      "\n",
      "reshape(tensor, shape, name=None)\n",
      "    Reshapes a tensor.\n",
      "    \n",
      "    Given `tensor`, this operation returns a tensor that has the same values\n",
      "    as `tensor` with shape `shape`.\n",
      "    \n",
      "    If one component of `shape` is the special value -1, the size of that dimension\n",
      "    is computed so that the total size remains constant.  In particular, a `shape`\n",
      "    of `[-1]` flattens into 1-D.  At most one component of `shape` can be -1.\n",
      "    \n",
      "    If `shape` is 1-D or higher, then the operation returns a tensor with shape\n",
      "    `shape` filled with the values of `tensor`. In this case, the number of elements\n",
      "    implied by `shape` must be the same as the number of elements in `tensor`.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```prettyprint\n",
      "    # tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "    # tensor 't' has shape [9]\n",
      "    reshape(t, [3, 3]) ==> [[1, 2, 3],\n",
      "                            [4, 5, 6],\n",
      "                            [7, 8, 9]]\n",
      "    \n",
      "    # tensor 't' is [[[1, 1], [2, 2]],\n",
      "    #                [[3, 3], [4, 4]]]\n",
      "    # tensor 't' has shape [2, 2, 2]\n",
      "    reshape(t, [2, 4]) ==> [[1, 1, 2, 2],\n",
      "                            [3, 3, 4, 4]]\n",
      "    \n",
      "    # tensor 't' is [[[1, 1, 1],\n",
      "    #                 [2, 2, 2]],\n",
      "    #                [[3, 3, 3],\n",
      "    #                 [4, 4, 4]],\n",
      "    #                [[5, 5, 5],\n",
      "    #                 [6, 6, 6]]]\n",
      "    # tensor 't' has shape [3, 2, 3]\n",
      "    # pass '[-1]' to flatten 't'\n",
      "    reshape(t, [-1]) ==> [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]\n",
      "    \n",
      "    # -1 can also be used to infer the shape\n",
      "    \n",
      "    # -1 is inferred to be 9:\n",
      "    reshape(t, [2, -1]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
      "                             [4, 4, 4, 5, 5, 5, 6, 6, 6]]\n",
      "    # -1 is inferred to be 2:\n",
      "    reshape(t, [-1, 9]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
      "                             [4, 4, 4, 5, 5, 5, 6, 6, 6]]\n",
      "    # -1 is inferred to be 3:\n",
      "    reshape(t, [ 2, -1, 3]) ==> [[[1, 1, 1],\n",
      "                                  [2, 2, 2],\n",
      "                                  [3, 3, 3]],\n",
      "                                 [[4, 4, 4],\n",
      "                                  [5, 5, 5],\n",
      "                                  [6, 6, 6]]]\n",
      "    \n",
      "    # tensor 't' is [7]\n",
      "    # shape `[]` reshapes to a scalar\n",
      "    reshape(t, []) ==> 7\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      tensor: A `Tensor`.\n",
      "      shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "        Defines the shape of the output tensor.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `tensor`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
